{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchaudio\n",
    "from comet_ml import Experiment\n",
    "from ctcdecode import CTCBeamDecoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FOLDER = 'LibriSpeech'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(dataset_folder, download=False):\n",
    "    if not os.path.isdir(\"./data\"):\n",
    "        os.makedirs(\"./data\")\n",
    "\n",
    "    train_dataset = torchaudio.datasets.LIBRISPEECH(root='./data', folder_in_archive=dataset_folder, url='train-clean-100', download=download)\n",
    "    test_dataset = torchaudio.datasets.LIBRISPEECH(root='./data', folder_in_archive=dataset_folder, url='test-clean', download=download)\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torchaudio.datasets.librispeech.LIBRISPEECH at 0x7f911ea13760>,\n",
       " <torchaudio.datasets.librispeech.LIBRISPEECH at 0x7f91fc0ff2e0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_data(DATASET_FOLDER, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28547 2620\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = download_data(DATASET_FOLDER)\n",
    "print(len(train_dataset), len(test_dataset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pigeon_gcc/anaconda3/envs/asr_project/lib/python3.9/site-packages/torchaudio/functional/functional.py:571: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "char_map_str = \"\"\"\n",
    "' 0\n",
    "<BLANK> 1\n",
    "a 2\n",
    "b 3\n",
    "c 4\n",
    "d 5\n",
    "e 6\n",
    "f 7\n",
    "g 8\n",
    "h 9\n",
    "i 10\n",
    "j 11\n",
    "k 12\n",
    "l 13\n",
    "m 14\n",
    "n 15\n",
    "o 16\n",
    "p 17\n",
    "q 18\n",
    "r 19\n",
    "s 20\n",
    "t 21\n",
    "u 22\n",
    "v 23\n",
    "w 24\n",
    "x 25\n",
    "y 26\n",
    "z 27\n",
    "\"\"\"\n",
    "\n",
    "BLANK_LABEL = None  # to be assigned in TextTranform constructor\n",
    "\n",
    "class TextTransform:\n",
    "    \"\"\" Maps characters to their indices, and vice versa \"\"\"\n",
    "    def __init__(self):\n",
    "        self.char_map = {}\n",
    "        self.index_map = {}\n",
    "        global BLANK_LABEL\n",
    "        for line in char_map_str.strip().split('\\n'):\n",
    "            ch, index = line.split()\n",
    "            self.char_map[ch] = int(index)\n",
    "            self.index_map[int(index)] = ch\n",
    "            if not BLANK_LABEL and ch == '<BLANK>':\n",
    "                BLANK_LABEL = int(index)\n",
    "        self.index_map[BLANK_LABEL] = ' '\n",
    "\n",
    "    def text_to_int(self, text: list[str]):\n",
    "        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
    "        int_sequence = []\n",
    "        for c in text:\n",
    "            if c == ' ':\n",
    "                ind = self.char_map['<BLANK>']\n",
    "            else:\n",
    "                ind = self.char_map[c]\n",
    "            int_sequence.append(ind)\n",
    "        return int_sequence\n",
    "\n",
    "    def int_to_text(self, labels: list[int]):\n",
    "        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
    "        string = []\n",
    "        for i in labels:\n",
    "            string.append(self.index_map[i])\n",
    "        return ''.join(string).replace('<BLANK>', ' ').strip()\n",
    "\n",
    "\n",
    "# TODO: SpecAugment (masking augmentations)\n",
    "train_audio_transforms = nn.Sequential(\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
    "    torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n",
    "    torchaudio.transforms.TimeMasking(time_mask_param=35)\n",
    ")\n",
    "\n",
    "valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
    "\n",
    "text_transform = TextTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(BLANK_LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes --> [26, 6, 20] --> yes\n"
     ]
    }
   ],
   "source": [
    "# testing the code above\n",
    "word_start = \"yes\"\n",
    "index = text_transform.text_to_int(word_start)\n",
    "word_recovered = text_transform.int_to_text(index)\n",
    "\n",
    "print(word_start, \"-->\", index, \"-->\", word_recovered)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function __data_processing()__ will be called in Data Loaders' __collate_fn__.\n",
    "\n",
    "Data is represented as tuple(wave, sample_rate, utterance (label), speaker id, utterance number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000,  0.0000,  0.0000,  ..., -0.0004, -0.0004, -0.0004]]),\n",
       " 16000,\n",
       " 'hello this is a test of our web application this is my first note',\n",
       " 10000,\n",
       " 9999999,\n",
       " 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = train_dataset.__getitem__(n=2)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(data, data_type=\"train\"):\n",
    "    spectrograms = []\n",
    "    labels = []\n",
    "    input_lengths = []\n",
    "    label_lengths = []\n",
    "    for (waveform, _, utterance, _, _, _) in data:\n",
    "        if data_type == 'train':\n",
    "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        else:\n",
    "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        spectrograms.append(spec)\n",
    "        # labels are lists of integer character ids\n",
    "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
    "        labels.append(label)\n",
    "        # input_lengths, label_lengths are used in loss function\n",
    "        input_lengths.append(spec.shape[0]//2)\n",
    "        label_lengths.append(len(label))\n",
    "\n",
    "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    return spectrograms, labels, input_lengths, label_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 6.2253e-14, 4.4080e-09,  ..., 2.0605e-02,\n",
       "            3.9127e-02, 3.7011e-02],\n",
       "           [0.0000e+00, 3.3519e-13, 2.3734e-08,  ..., 1.1094e-01,\n",
       "            2.1067e-01, 1.9928e-01],\n",
       "           ...,\n",
       "           [0.0000e+00, 4.5395e-14, 7.4438e-12,  ..., 2.7929e-07,\n",
       "            3.0889e-07, 8.6704e-08],\n",
       "           [0.0000e+00, 7.5903e-14, 1.1712e-11,  ..., 2.1022e-07,\n",
       "            1.8068e-07, 1.6802e-07],\n",
       "           [0.0000e+00, 7.0859e-14, 3.4167e-11,  ..., 2.0119e-07,\n",
       "            1.0049e-07, 4.4200e-08]]]]),\n",
       " tensor([[ 9.,  6., 13., 13., 16.,  1., 21.,  9., 10., 20.,  1., 10., 20.,  1.,\n",
       "           2.,  1., 21.,  6., 20., 21.,  1., 16.,  7.,  1., 16., 22., 19.,  1.,\n",
       "          24.,  6.,  3.,  1.,  2., 17., 17., 13., 10.,  4.,  2., 21., 10., 16.,\n",
       "          15.,  1., 21.,  9., 10., 20.,  1., 10., 20.,  1., 14., 26.,  1.,  7.,\n",
       "          10., 19., 20., 21.,  1., 15., 16., 21.,  6.]]),\n",
       " [347],\n",
       " [65])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing\n",
    "data_processing((sample,))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CER and WER metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _levenshtein_distance(ref, hyp):\n",
    "    \"\"\"Levenshtein distance is a string metric for measuring the difference\n",
    "    between two sequences. Informally, the levenshtein disctance is defined as\n",
    "    the minimum number of single-character edits (substitutions, insertions or\n",
    "    deletions) required to change one word into the other. We can naturally\n",
    "    extend the edits to word level when calculate levenshtein disctance for\n",
    "    two sentences.\n",
    "    \"\"\"\n",
    "    m = len(ref)\n",
    "    n = len(hyp)\n",
    "\n",
    "    # special case\n",
    "    if ref == hyp:\n",
    "        return 0\n",
    "    if m == 0:\n",
    "        return n\n",
    "    if n == 0:\n",
    "        return m\n",
    "\n",
    "    if m < n:\n",
    "        ref, hyp = hyp, ref\n",
    "        m, n = n, m\n",
    "\n",
    "    # use O(min(m, n)) space\n",
    "    distance = np.zeros((2, n + 1), dtype=np.int32)\n",
    "\n",
    "    # initialize distance matrix\n",
    "    for j in range(0,n + 1):\n",
    "        distance[0][j] = j\n",
    "\n",
    "    # calculate levenshtein distance\n",
    "    for i in range(1, m + 1):\n",
    "        prev_row_idx = (i - 1) % 2\n",
    "        cur_row_idx = i % 2\n",
    "        distance[cur_row_idx][0] = i\n",
    "        for j in range(1, n + 1):\n",
    "            if ref[i - 1] == hyp[j - 1]:\n",
    "                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n",
    "            else:\n",
    "                s_num = distance[prev_row_idx][j - 1] + 1\n",
    "                i_num = distance[cur_row_idx][j - 1] + 1\n",
    "                d_num = distance[prev_row_idx][j] + 1\n",
    "                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n",
    "\n",
    "    return distance[m % 2][n]\n",
    "\n",
    "\n",
    "def word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n",
    "    \"\"\"Compute the levenshtein distance between reference sequence and\n",
    "    hypothesis sequence in word-level.\n",
    "    :param reference: The reference sentence.\n",
    "    :type reference: basestring\n",
    "    :param hypothesis: The hypothesis sentence.\n",
    "    :type hypothesis: basestring\n",
    "    :param ignore_case: Whether case-sensitive or not.\n",
    "    :type ignore_case: bool\n",
    "    :param delimiter: Delimiter of input sentences.\n",
    "    :type delimiter: char\n",
    "    :return: Levenshtein distance and word number of reference sentence.\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    if ignore_case == True:\n",
    "        reference = reference.lower()\n",
    "        hypothesis = hypothesis.lower()\n",
    "\n",
    "    ref_words = reference.split(delimiter)\n",
    "    hyp_words = hypothesis.split(delimiter)\n",
    "\n",
    "    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n",
    "    return float(edit_distance), len(ref_words)\n",
    "\n",
    "\n",
    "def char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\n",
    "    \"\"\"Compute the levenshtein distance between reference sequence and\n",
    "    hypothesis sequence in char-level.\n",
    "    :param reference: The reference sentence.\n",
    "    :type reference: basestring\n",
    "    :param hypothesis: The hypothesis sentence.\n",
    "    :type hypothesis: basestring\n",
    "    :param ignore_case: Whether case-sensitive or not.\n",
    "    :type ignore_case: bool\n",
    "    :param remove_space: Whether remove internal space characters\n",
    "    :type remove_space: bool\n",
    "    :return: Levenshtein distance and length of reference sentence.\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    if ignore_case == True:\n",
    "        reference = reference.lower()\n",
    "        hypothesis = hypothesis.lower()\n",
    "\n",
    "    join_char = ' '\n",
    "    if remove_space == True:\n",
    "        join_char = ''\n",
    "\n",
    "    reference = join_char.join(filter(None, reference.split(' ')))\n",
    "    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n",
    "\n",
    "    edit_distance = _levenshtein_distance(reference, hypothesis)\n",
    "    return float(edit_distance), len(reference)\n",
    "\n",
    "\n",
    "def wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n",
    "    \"\"\"Calculate word error rate (WER). WER compares reference text and\n",
    "    hypothesis text in word-level. WER is defined as:\n",
    "    .. math::\n",
    "        WER = (Sw + Dw + Iw) / Nw\n",
    "    where\n",
    "    .. code-block:: text\n",
    "        Sw is the number of words subsituted,\n",
    "        Dw is the number of words deleted,\n",
    "        Iw is the number of words inserted,\n",
    "        Nw is the number of words in the reference\n",
    "    We can use levenshtein distance to calculate WER. Please draw an attention\n",
    "    that empty items will be removed when splitting sentences by delimiter.\n",
    "    :param reference: The reference sentence.\n",
    "    :type reference: basestring\n",
    "    :param hypothesis: The hypothesis sentence.\n",
    "    :type hypothesis: basestring\n",
    "    :param ignore_case: Whether case-sensitive or not.\n",
    "    :type ignore_case: bool\n",
    "    :param delimiter: Delimiter of input sentences.\n",
    "    :type delimiter: char\n",
    "    :return: Word error rate.\n",
    "    :rtype: float\n",
    "    :raises ValueError: If word number of reference is zero.\n",
    "    \"\"\"\n",
    "    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case,\n",
    "                                         delimiter)\n",
    "\n",
    "    if ref_len == 0:\n",
    "        raise ValueError(\"Reference's word number should be greater than 0.\")\n",
    "\n",
    "    wer = float(edit_distance) / ref_len\n",
    "    return wer\n",
    "\n",
    "\n",
    "def cer(reference, hypothesis, ignore_case=False, remove_space=False):\n",
    "    \"\"\"Calculate charactor error rate (CER). CER compares reference text and\n",
    "    hypothesis text in char-level. CER is defined as:\n",
    "    .. math::\n",
    "        CER = (Sc + Dc + Ic) / Nc\n",
    "    where\n",
    "    .. code-block:: text\n",
    "        Sc is the number of characters substituted,\n",
    "        Dc is the number of characters deleted,\n",
    "        Ic is the number of characters inserted\n",
    "        Nc is the number of characters in the reference\n",
    "    We can use levenshtein distance to calculate CER. Chinese input should be\n",
    "    encoded to unicode. Please draw an attention that the leading and tailing\n",
    "    space characters will be truncated and multiple consecutive space\n",
    "    characters in a sentence will be replaced by one space character.\n",
    "    :param reference: The reference sentence.\n",
    "    :type reference: basestring\n",
    "    :param hypothesis: The hypothesis sentence.\n",
    "    :type hypothesis: basestring\n",
    "    :param ignore_case: Whether case-sensitive or not.\n",
    "    :type ignore_case: bool\n",
    "    :param remove_space: Whether remove internal space characters\n",
    "    :type remove_space: bool\n",
    "    :return: Character error rate.\n",
    "    :rtype: float\n",
    "    :raises ValueError: If the reference length is zero.\n",
    "    \"\"\"\n",
    "    edit_distance, ref_len = char_errors(reference, hypothesis, ignore_case,\n",
    "                                         remove_space)\n",
    "\n",
    "    if ref_len == 0:\n",
    "        raise ValueError(\"Length of reference should be greater than 0.\")\n",
    "\n",
    "    cer = float(edit_distance) / ref_len\n",
    "    return cer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Layer Normalization, not Batch Normalization, because BN is hard to use with sequence data, with small batch sizes, and it's hard to paralellize a NN with BN.\n",
    "\n",
    "This is due to the dependency on batches. Layer Normalization removes this dependency. It computes the normalization based on the layers inside of the batches.\n",
    "\n",
    "LN briefly: Input values in all neurons in the same layer are normalized for each data sample.\n",
    "So, all values in neurons of the same layer will have the same mean and variance.\n",
    "\n",
    "LN is can deal with sequence data, doesn't depend on batch size, and is easily paralellized.\n",
    "However, LN sometimes performs worse than BN with CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLayerNorm(nn.Module):\n",
    "    \"\"\"Layer Normalization\"\"\"\n",
    "    \n",
    "    def __init__(self, n_features):\n",
    "        super(CNNLayerNorm, self).__init__()\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=n_features)\n",
    "        \"\"\"About normalized_shape parameter of nn.LayerNorm:\n",
    "        If a single integer is used, it is treated as a singleton list, and this module will normalize\n",
    "        over the last dimension which is expected to be of that specific size.\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x (batch, channel, feature, time)\n",
    "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
    "        x = self.layer_norm(x)\n",
    "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n",
    "\n",
    "class ResidualCNN(nn.Module):\n",
    "    \"\"\" Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
    "        except with layer norm instead of batch norm \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_features):\n",
    "        super(ResidualCNN, self).__init__()\n",
    "\n",
    "        self.layer_norm1 = CNNLayerNorm(n_features)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "\n",
    "        self.layer_norm2 = CNNLayerNorm(n_features)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x  # (batch, channel, feature, time)\n",
    "        out = self.layer_norm1(x)\n",
    "        out = F.gelu(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.cnn1(out)\n",
    "        out = self.layer_norm2(out)\n",
    "        out = F.gelu(out)\n",
    "        out = self.dropout2(out)\n",
    "        out = self.cnn2(out)\n",
    "        out += residual\n",
    "        return out # (batch, channel, feature, time)\n",
    "        \n",
    "class BidirectionalGRU(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_type, rnn_dim, hidden_size, dropout, batch_first):\n",
    "        super(BidirectionalGRU, self).__init__()\n",
    "\n",
    "        if rnn_type == \"GRU\":\n",
    "            self.rnn_cell = nn.GRU\n",
    "        elif rnn_type == \"LSTM\":\n",
    "            self.rnn_cell = nn.LSTM\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
    "        self.BiGRU = self.rnn_cell(\n",
    "            input_size=rnn_dim, hidden_size=hidden_size,\n",
    "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer_norm(x)\n",
    "        out = F.gelu(out)\n",
    "        out, _ = self.BiGRU(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SpeechRecognitionModel(nn.Module):\n",
    "    \"\"\"Speech Recognition Model Inspired by DeepSpeech 2\"\"\"\n",
    "\n",
    "    def __init__(self, rnn_type, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_features, stride=2, dropout=0.1):\n",
    "        super(SpeechRecognitionModel, self).__init__()\n",
    "        n_features = n_features // 2\n",
    "\n",
    "        # TODO: purpose of this conv layer\n",
    "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3 // 2)  # cnn for extracting heirachal features\n",
    "\n",
    "        # n residual cnn layers with filter size of 32\n",
    "        self.rescnn_layers = nn.Sequential(*[\n",
    "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_features=n_features) \n",
    "            for _ in range(n_cnn_layers)\n",
    "        ])\n",
    "        self.fully_connected = nn.Linear(n_features*32, rnn_dim)\n",
    "        \"\"\"TODO: как я понял, у нас число фичей rnn_dim берётся одиночное для первой GRU, т.к. к ней не перетекает скрытое состояние.\n",
    "        Каждая следующая GRU получает 2*rnn_dim фичей, т.к. к самим фичам конкатенируется скрытое состояние такой же размерности (hidden_size=rnn_dim)\n",
    "        \"\"\"\n",
    "        self.birnn_layers = nn.Sequential(*[\n",
    "            BidirectionalGRU(rnn_type=rnn_type, \n",
    "                             rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
    "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
    "            for i in range(n_rnn_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(rnn_dim, n_class)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.rescnn_layers(x)\n",
    "        sizes = x.size()\n",
    "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
    "        x = x.transpose(1, 2) # (batch, time, feature)\n",
    "        x = self.fully_connected(x)\n",
    "        x = self.birnn_layers(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GreedyDecoder(output, labels, label_lengths, blank_label=BLANK_LABEL, collapse_repeated=True):\n",
    "    output = F.log_softmax(output, dim=2)\n",
    "\n",
    "    arg_maxes = torch.argmax(output, dim=2)\n",
    "    decodes = []\n",
    "    targets = []\n",
    "    \n",
    "    for i, args in enumerate(arg_maxes):\n",
    "        decode = []\n",
    "        targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
    "        blank_ctr = 0\n",
    "        for j, index in enumerate(args):\n",
    "            if collapse_repeated and index == blank_label:\n",
    "                blank_ctr += 1\n",
    "            else:\n",
    "                if blank_ctr > 2:\n",
    "                    decode.append(blank_label)\n",
    "                blank_ctr = 0\n",
    "\n",
    "                if collapse_repeated and j != 0 and index == args[j - 1]:\n",
    "                    continue\n",
    "                \n",
    "                decode.append(index.item())\n",
    "        \n",
    "        decodes.append(text_transform.int_to_text(decode))\n",
    "\n",
    "    return decodes, targets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = list(text_transform.char_map.keys())\n",
    "\n",
    "beam_decoder = CTCBeamDecoder(\n",
    "    labels=characters,\n",
    "    cutoff_top_n=len(characters),   # do not discard any characters from beam search\n",
    "    cutoff_prob=1.0,    # cutoff probability in pruning (1.0 means no pruning)\n",
    "    beam_width=100,\n",
    "    num_processes=7,\n",
    "    blank_id=28,\n",
    "    log_probs_input=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BeamSearchDecoder(output):\n",
    "    top_beams = []\n",
    "    decodings = []\n",
    "\n",
    "    beam_results, beam_scores, timesteps, out_lens = beam_decoder.decode(output)\n",
    "    \n",
    "    for i in range(len(beam_results)):\n",
    "        top_beam = beam_results[i][0][:out_lens[i][0]]\n",
    "        decoding = text_transform.int_to_text(top_beam.tolist())\n",
    "\n",
    "        top_beams.append(top_beam)\n",
    "        decodings.append(decoding)\n",
    "        \n",
    "    return top_beams, decodings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    torch.manual_seed(7)\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    return device, use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model,\n",
    "          sample_idx: int = None,\n",
    "          sample_path: int = None,\n",
    "          collapse_repeated: bool = True,\n",
    "          dataset = test_dataset):\n",
    "    device, _ = get_device()\n",
    "\n",
    "    if sample_path is not None:\n",
    "        waveform, _ = torchaudio.load(sample_path)\n",
    "        sample = (waveform, None, \"\", None, None, None)\n",
    "    elif sample_idx is not None:\n",
    "        sample = dataset[sample_idx]\n",
    "\n",
    "    spectrogram, label, input_length, label_length = \\\n",
    "        data_processing((sample,))\n",
    "    spectrogram, label = spectrogram.to(device), label.to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    output = model(spectrogram)\n",
    "\n",
    "    greedy_pred, label = GreedyDecoder(output, label, label_length, collapse_repeated=collapse_repeated)\n",
    "    beams, beam_preds = BeamSearchDecoder(output)\n",
    "    \n",
    "    print(f\"Negative log likelihood matrix shape: {output.shape}\")\n",
    "    print(\"\\nGREEDY DECODING\")\n",
    "    print(f\"Decoded indices:\\n{torch.argmax(output, dim=2)}\")\n",
    "    print()\n",
    "    print(f\"Target (len {len(label[0])}): {label}\")\n",
    "    print(f\"Prediction (len {len(greedy_pred[0])}): {greedy_pred}\")\n",
    "\n",
    "    print(\"\\nBEAM SEARCH DECODING\")\n",
    "    print(f\"Top beam:\\n{beams}\")\n",
    "    print()\n",
    "    print(f\"Target (len {len(label[0])}): {label}\")\n",
    "    print(f\"Prediction (len {len(beam_preds[0])}): {beam_preds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n"
     ]
    }
   ],
   "source": [
    "class IterMeter(object):\n",
    "    \"\"\"Keeps track of total iterations. Used for Comet.ml\"\"\"\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.val += 1\n",
    "\n",
    "    def get(self):\n",
    "        return self.val\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, experiment):\n",
    "    model.train()\n",
    "    data_len = len(train_loader.dataset)\n",
    "    with experiment.train():\n",
    "        for batch_idx, _data in enumerate(train_loader):\n",
    "            spectrograms, labels, input_lengths, label_lengths = _data \n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(spectrograms)  # (batch, time, n_class)\n",
    "            output = F.log_softmax(output, dim=2)\n",
    "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "            loss.backward()\n",
    "\n",
    "            experiment.log_metric('loss', loss.item(), step=iter_meter.get())\n",
    "            experiment.log_metric('learning_rate', scheduler.get_last_lr(), step=iter_meter.get())\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()   # scheduler step for oncecycle scheduler\n",
    "            iter_meter.step()\n",
    "\n",
    "            if batch_idx % 100 == 0 or batch_idx == data_len:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(spectrograms), data_len,\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, criterion, epoch, iter_meter, experiment):\n",
    "    print('\\nEvaluating…')\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    test_cer_greedy, test_wer_greedy = [], []\n",
    "    test_cer_beam, test_wer_beam = [], []\n",
    "\n",
    "    with experiment.test():\n",
    "        with torch.no_grad():\n",
    "            for I, _data in enumerate(test_loader):\n",
    "                spectrograms, labels, input_lengths, label_lengths = _data \n",
    "                spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "                output = model(spectrograms)  # (batch, time, n_class)\n",
    "                # output = F.log_softmax(output, dim=2)\n",
    "                # output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "                output_for_loss = F.log_softmax(output, dim=2).transpose(0, 1)\n",
    "                loss = criterion(output_for_loss, labels, input_lengths, label_lengths)\n",
    "                test_loss += loss.item() / len(test_loader)\n",
    "\n",
    "                greedy_preds, decoded_targets = GreedyDecoder(output, labels, label_lengths)\n",
    "                beams, beam_preds = BeamSearchDecoder(output)\n",
    "\n",
    "                for j in range(len(greedy_preds)):\n",
    "                    test_cer_greedy.append(cer(decoded_targets[j], greedy_preds[j]))\n",
    "                    test_wer_greedy.append(wer(decoded_targets[j], greedy_preds[j]))\n",
    "\n",
    "                for j in range(len(beam_preds)):\n",
    "                    test_cer_beam.append(cer(decoded_targets[j], beam_preds[j]))\n",
    "                    test_wer_beam.append(wer(decoded_targets[j], beam_preds[j]))\n",
    "\n",
    "    experiment.log_metric('test_loss', test_loss, step=iter_meter.get())\n",
    "    print('Test set: Average loss: {:.4f}'.format(test_loss))\n",
    "\n",
    "    avg_cer_greedy = sum(test_cer_greedy)/len(test_cer_greedy)\n",
    "    avg_wer_greedy = sum(test_wer_greedy)/len(test_wer_greedy)\n",
    "    experiment.log_metric('cer_greedy', avg_cer_greedy, step=iter_meter.get())\n",
    "    experiment.log_metric('wer_greedy', avg_wer_greedy, step=iter_meter.get())\n",
    "    print('Average Greedy CER: {:4f} Average Greedy WER: {:.4f}\\n'.format(avg_cer_greedy, avg_wer_greedy))\n",
    "\n",
    "    avg_cer_beam = sum(test_cer_beam)/len(test_cer_beam)\n",
    "    avg_wer_beam = sum(test_wer_beam)/len(test_wer_beam)\n",
    "    experiment.log_metric('cer_beam', avg_cer_beam, step=iter_meter.get())\n",
    "    experiment.log_metric('wer_beam', avg_wer_beam, step=iter_meter.get())\n",
    "    print('Average Beam CER: {:4f} Average Beam WER: {:.4f}\\n'.format(avg_cer_beam, avg_wer_beam))\n",
    "\n",
    "    infer(model, sample_idx=2000)\n",
    "    infer(model, sample_idx=0, dataset=train_dataset)\n",
    "\n",
    "\n",
    "def train_test(hparams, experiment=Experiment(api_key='dummy_key', disabled=True)):\n",
    "\n",
    "    experiment.log_parameters(hparams)\n",
    "\n",
    "    train_dataset, test_dataset = download_data(DATASET_FOLDER)\n",
    "\n",
    "    device, use_cuda = get_device()\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "    train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                                batch_size=hparams['batch_size'],\n",
    "                                shuffle=True,\n",
    "                                collate_fn=lambda x: data_processing(x, 'train'),\n",
    "                                **kwargs)\n",
    "    test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                                batch_size=hparams['batch_size'],\n",
    "                                shuffle=False,\n",
    "                                collate_fn=lambda x: data_processing(x, 'test'),\n",
    "                                **kwargs)\n",
    "\n",
    "    model = SpeechRecognitionModel(\n",
    "        hparams['rnn_type'], hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
    "        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
    "        ).to(device)\n",
    "\n",
    "    print(model)\n",
    "    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
    "    criterion = nn.CTCLoss(blank=BLANK_LABEL).to(device)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n",
    "                                            steps_per_epoch=int(len(train_loader)),\n",
    "                                            epochs=hparams['epochs'],\n",
    "                                            anneal_strategy='linear')\n",
    "\n",
    "    iter_meter = IterMeter()\n",
    "    for epoch in range(1, hparams['epochs'] + 1):\n",
    "        train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, experiment)\n",
    "        test(model, device, test_loader, criterion, epoch, iter_meter, experiment)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiments are monitored using Comet.ml:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: torch. Metrics and hyperparameters can still be logged using Experiment.log_metrics() and Experiment.log_parameters()\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.com https://www.comet.com/pigeongcc/speech-recognition/42f087c969e5452f9adada1a8f830f74\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hparams = {\n",
    "    \"rnn_type\": \"LSTM\",\n",
    "    \"n_cnn_layers\": 4,\n",
    "    \"n_rnn_layers\": 7,\n",
    "    \"n_feats\": 256,\n",
    "    \"stride\": 1,\n",
    "    \"rnn_dim\": 256,\n",
    "    \"n_class\": len(characters),\n",
    "    \"dropout\": 0.05,\n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"batch_size\": 3,\n",
    "    \"epochs\": 10,\n",
    "}\n",
    "\n",
    "comet_api_key = \"KddYvSKPDO9U8K1lZUIUCgHjT\"\n",
    "project_name = \"speech-recognition\"\n",
    "experiment_name = f\"epochs:{hparams['epochs']}-rnn_type:{hparams['rnn_type']}-rnn_dim:{hparams['rnn_dim']}-n_rnn_layers:{hparams['n_rnn_layers']}-n_cnn_layers:{hparams['n_cnn_layers']}\"\n",
    "\n",
    "if comet_api_key:\n",
    "    experiment = Experiment(api_key=comet_api_key, project_name=project_name, parse_args=False)\n",
    "    experiment.set_name(experiment_name)\n",
    "    # experiment.display()\n",
    "else:\n",
    "    experiment = Experiment(api_key='dummy_key', disabled=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpeechRecognitionModel(\n",
      "  (cnn): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (rescnn_layers): Sequential(\n",
      "    (0): ResidualCNN(\n",
      "      (layer_norm1): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.05, inplace=False)\n",
      "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (layer_norm2): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.05, inplace=False)\n",
      "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): ResidualCNN(\n",
      "      (layer_norm1): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.05, inplace=False)\n",
      "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (layer_norm2): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.05, inplace=False)\n",
      "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (2): ResidualCNN(\n",
      "      (layer_norm1): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.05, inplace=False)\n",
      "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (layer_norm2): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.05, inplace=False)\n",
      "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (3): ResidualCNN(\n",
      "      (layer_norm1): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (dropout1): Dropout(p=0.05, inplace=False)\n",
      "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (layer_norm2): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (dropout2): Dropout(p=0.05, inplace=False)\n",
      "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (fully_connected): Linear(in_features=4096, out_features=256, bias=True)\n",
      "  (birnn_layers): Sequential(\n",
      "    (0): BidirectionalGRU(\n",
      "      (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (BiGRU): LSTM(256, 256, batch_first=True, bidirectional=True)\n",
      "      (dropout): Dropout(p=0.05, inplace=False)\n",
      "    )\n",
      "    (1): BidirectionalGRU(\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (BiGRU): LSTM(512, 256, bidirectional=True)\n",
      "      (dropout): Dropout(p=0.05, inplace=False)\n",
      "    )\n",
      "    (2): BidirectionalGRU(\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (BiGRU): LSTM(512, 256, bidirectional=True)\n",
      "      (dropout): Dropout(p=0.05, inplace=False)\n",
      "    )\n",
      "    (3): BidirectionalGRU(\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (BiGRU): LSTM(512, 256, bidirectional=True)\n",
      "      (dropout): Dropout(p=0.05, inplace=False)\n",
      "    )\n",
      "    (4): BidirectionalGRU(\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (BiGRU): LSTM(512, 256, bidirectional=True)\n",
      "      (dropout): Dropout(p=0.05, inplace=False)\n",
      "    )\n",
      "    (5): BidirectionalGRU(\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (BiGRU): LSTM(512, 256, bidirectional=True)\n",
      "      (dropout): Dropout(p=0.05, inplace=False)\n",
      "    )\n",
      "    (6): BidirectionalGRU(\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (BiGRU): LSTM(512, 256, bidirectional=True)\n",
      "      (dropout): Dropout(p=0.05, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Dropout(p=0.05, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=28, bias=True)\n",
      "  )\n",
      ")\n",
      "Num Model Parameters 11784796\n",
      "Train Epoch: 1 [0/28547 (0%)]\tLoss: 6.907328\n",
      "Train Epoch: 1 [300/28547 (1%)]\tLoss: 2.982194\n",
      "Train Epoch: 1 [600/28547 (2%)]\tLoss: 2.128279\n",
      "Train Epoch: 1 [900/28547 (3%)]\tLoss: 2.175375\n",
      "Train Epoch: 1 [1200/28547 (4%)]\tLoss: 2.111467\n",
      "Train Epoch: 1 [1500/28547 (5%)]\tLoss: 2.060259\n",
      "Train Epoch: 1 [1800/28547 (6%)]\tLoss: 2.077214\n",
      "Train Epoch: 1 [2100/28547 (7%)]\tLoss: 2.094412\n",
      "Train Epoch: 1 [2400/28547 (8%)]\tLoss: 2.078908\n",
      "Train Epoch: 1 [2700/28547 (9%)]\tLoss: 2.096459\n",
      "Train Epoch: 1 [3000/28547 (11%)]\tLoss: 2.265830\n",
      "Train Epoch: 1 [3300/28547 (12%)]\tLoss: 2.139497\n",
      "Train Epoch: 1 [3600/28547 (13%)]\tLoss: 2.053165\n",
      "Train Epoch: 1 [3900/28547 (14%)]\tLoss: 2.052754\n",
      "Train Epoch: 1 [4200/28547 (15%)]\tLoss: 2.046499\n",
      "Train Epoch: 1 [4500/28547 (16%)]\tLoss: 2.077533\n",
      "Train Epoch: 1 [4800/28547 (17%)]\tLoss: 2.073783\n",
      "Train Epoch: 1 [5100/28547 (18%)]\tLoss: 2.096992\n",
      "Train Epoch: 1 [5400/28547 (19%)]\tLoss: 2.086671\n",
      "Train Epoch: 1 [5700/28547 (20%)]\tLoss: 2.206665\n",
      "Train Epoch: 1 [6000/28547 (21%)]\tLoss: 2.120613\n",
      "Train Epoch: 1 [6300/28547 (22%)]\tLoss: 1.988100\n",
      "Train Epoch: 1 [6600/28547 (23%)]\tLoss: 2.116177\n",
      "Train Epoch: 1 [6900/28547 (24%)]\tLoss: 2.181149\n",
      "Train Epoch: 1 [7200/28547 (25%)]\tLoss: 2.085128\n",
      "Train Epoch: 1 [7500/28547 (26%)]\tLoss: 2.122669\n",
      "Train Epoch: 1 [7800/28547 (27%)]\tLoss: 2.064395\n",
      "Train Epoch: 1 [8100/28547 (28%)]\tLoss: 2.050108\n",
      "Train Epoch: 1 [8400/28547 (29%)]\tLoss: 2.044493\n",
      "Train Epoch: 1 [8700/28547 (30%)]\tLoss: 2.097708\n",
      "Train Epoch: 1 [9000/28547 (32%)]\tLoss: 2.012415\n",
      "Train Epoch: 1 [9300/28547 (33%)]\tLoss: 2.203785\n",
      "Train Epoch: 1 [9600/28547 (34%)]\tLoss: 1.983561\n",
      "Train Epoch: 1 [9900/28547 (35%)]\tLoss: 2.009720\n",
      "Train Epoch: 1 [10200/28547 (36%)]\tLoss: 2.094216\n",
      "Train Epoch: 1 [10500/28547 (37%)]\tLoss: 1.972925\n",
      "Train Epoch: 1 [10800/28547 (38%)]\tLoss: 2.135914\n",
      "Train Epoch: 1 [11100/28547 (39%)]\tLoss: 2.080867\n",
      "Train Epoch: 1 [11400/28547 (40%)]\tLoss: 2.089250\n",
      "Train Epoch: 1 [11700/28547 (41%)]\tLoss: 2.147299\n",
      "Train Epoch: 1 [12000/28547 (42%)]\tLoss: 2.105473\n",
      "Train Epoch: 1 [12300/28547 (43%)]\tLoss: 2.173690\n",
      "Train Epoch: 1 [12600/28547 (44%)]\tLoss: 2.121827\n",
      "Train Epoch: 1 [12900/28547 (45%)]\tLoss: 2.102669\n",
      "Train Epoch: 1 [13200/28547 (46%)]\tLoss: 2.133554\n",
      "Train Epoch: 1 [13500/28547 (47%)]\tLoss: 2.126298\n",
      "Train Epoch: 1 [13800/28547 (48%)]\tLoss: 2.083641\n",
      "Train Epoch: 1 [14100/28547 (49%)]\tLoss: 2.153254\n",
      "Train Epoch: 1 [14400/28547 (50%)]\tLoss: 2.080733\n",
      "Train Epoch: 1 [14700/28547 (51%)]\tLoss: 2.072399\n",
      "Train Epoch: 1 [15000/28547 (53%)]\tLoss: 2.091578\n",
      "Train Epoch: 1 [15300/28547 (54%)]\tLoss: 2.212046\n",
      "Train Epoch: 1 [15600/28547 (55%)]\tLoss: 2.171381\n",
      "Train Epoch: 1 [15900/28547 (56%)]\tLoss: 2.082765\n",
      "Train Epoch: 1 [16200/28547 (57%)]\tLoss: 1.987538\n",
      "Train Epoch: 1 [16500/28547 (58%)]\tLoss: 2.270939\n",
      "Train Epoch: 1 [16800/28547 (59%)]\tLoss: 2.038042\n",
      "Train Epoch: 1 [17100/28547 (60%)]\tLoss: 1.992721\n",
      "Train Epoch: 1 [17400/28547 (61%)]\tLoss: 2.351693\n",
      "Train Epoch: 1 [17700/28547 (62%)]\tLoss: 2.186611\n",
      "Train Epoch: 1 [18000/28547 (63%)]\tLoss: 2.131820\n",
      "Train Epoch: 1 [18300/28547 (64%)]\tLoss: 2.040890\n",
      "Train Epoch: 1 [18600/28547 (65%)]\tLoss: 2.159602\n",
      "Train Epoch: 1 [18900/28547 (66%)]\tLoss: 2.070511\n",
      "Train Epoch: 1 [19200/28547 (67%)]\tLoss: 2.005801\n",
      "Train Epoch: 1 [19500/28547 (68%)]\tLoss: 2.104809\n",
      "Train Epoch: 1 [19800/28547 (69%)]\tLoss: 1.964732\n",
      "Train Epoch: 1 [20100/28547 (70%)]\tLoss: 2.003027\n",
      "Train Epoch: 1 [20400/28547 (71%)]\tLoss: 2.171331\n",
      "Train Epoch: 1 [20700/28547 (73%)]\tLoss: 2.026058\n",
      "Train Epoch: 1 [21000/28547 (74%)]\tLoss: 2.125510\n",
      "Train Epoch: 1 [21300/28547 (75%)]\tLoss: 2.059793\n",
      "Train Epoch: 1 [21600/28547 (76%)]\tLoss: 2.188361\n",
      "Train Epoch: 1 [21900/28547 (77%)]\tLoss: 2.043819\n",
      "Train Epoch: 1 [22200/28547 (78%)]\tLoss: 2.093616\n",
      "Train Epoch: 1 [22500/28547 (79%)]\tLoss: 2.113595\n",
      "Train Epoch: 1 [22800/28547 (80%)]\tLoss: 2.159882\n",
      "Train Epoch: 1 [23100/28547 (81%)]\tLoss: 2.063587\n",
      "Train Epoch: 1 [23400/28547 (82%)]\tLoss: 2.094537\n",
      "Train Epoch: 1 [23700/28547 (83%)]\tLoss: 1.977617\n",
      "Train Epoch: 1 [24000/28547 (84%)]\tLoss: 1.965705\n",
      "Train Epoch: 1 [24300/28547 (85%)]\tLoss: 2.081181\n",
      "Train Epoch: 1 [24600/28547 (86%)]\tLoss: 2.096865\n",
      "Train Epoch: 1 [24900/28547 (87%)]\tLoss: 2.011494\n",
      "Train Epoch: 1 [25200/28547 (88%)]\tLoss: 2.145552\n",
      "Train Epoch: 1 [25500/28547 (89%)]\tLoss: 2.126989\n",
      "Train Epoch: 1 [25800/28547 (90%)]\tLoss: 2.135486\n",
      "Train Epoch: 1 [26100/28547 (91%)]\tLoss: 2.078406\n",
      "Train Epoch: 1 [26400/28547 (92%)]\tLoss: 2.166441\n",
      "Train Epoch: 1 [26700/28547 (94%)]\tLoss: 2.443418\n",
      "Train Epoch: 1 [27000/28547 (95%)]\tLoss: 2.060968\n",
      "Train Epoch: 1 [27300/28547 (96%)]\tLoss: 2.094183\n",
      "Train Epoch: 1 [27600/28547 (97%)]\tLoss: 1.993470\n",
      "Train Epoch: 1 [27900/28547 (98%)]\tLoss: 2.169409\n",
      "Train Epoch: 1 [28200/28547 (99%)]\tLoss: 2.170831\n",
      "Train Epoch: 1 [28500/28547 (100%)]\tLoss: 2.094959\n",
      "\n",
      "Evaluating…\n",
      "Test set: Average loss: 2.1376\n",
      "Average Greedy CER: 0.968993 Average Greedy WER: 0.9996\n",
      "\n",
      "Average Beam CER: 0.959043 Average Beam WER: 1.0003\n",
      "\n",
      "Train Epoch: 2 [0/28547 (0%)]\tLoss: 2.157917\n",
      "Train Epoch: 2 [300/28547 (1%)]\tLoss: 2.015865\n",
      "Train Epoch: 2 [600/28547 (2%)]\tLoss: 1.931691\n",
      "Train Epoch: 2 [900/28547 (3%)]\tLoss: 2.003375\n",
      "Train Epoch: 2 [1200/28547 (4%)]\tLoss: 2.205642\n",
      "Train Epoch: 2 [1500/28547 (5%)]\tLoss: 2.178052\n",
      "Train Epoch: 2 [1800/28547 (6%)]\tLoss: 2.057751\n",
      "Train Epoch: 2 [2100/28547 (7%)]\tLoss: 2.053987\n",
      "Train Epoch: 2 [2400/28547 (8%)]\tLoss: 2.134651\n",
      "Train Epoch: 2 [2700/28547 (9%)]\tLoss: 2.096195\n",
      "Train Epoch: 2 [3000/28547 (11%)]\tLoss: 2.122672\n",
      "Train Epoch: 2 [3300/28547 (12%)]\tLoss: 2.001003\n",
      "Train Epoch: 2 [3600/28547 (13%)]\tLoss: 1.966292\n",
      "Train Epoch: 2 [3900/28547 (14%)]\tLoss: 2.146699\n",
      "Train Epoch: 2 [4200/28547 (15%)]\tLoss: 2.052860\n",
      "Train Epoch: 2 [4500/28547 (16%)]\tLoss: 2.099772\n",
      "Train Epoch: 2 [4800/28547 (17%)]\tLoss: 2.133262\n",
      "Train Epoch: 2 [5100/28547 (18%)]\tLoss: 2.084553\n",
      "Train Epoch: 2 [5400/28547 (19%)]\tLoss: 2.046957\n",
      "Train Epoch: 2 [5700/28547 (20%)]\tLoss: 2.071666\n",
      "Train Epoch: 2 [6000/28547 (21%)]\tLoss: 2.192753\n",
      "Train Epoch: 2 [6300/28547 (22%)]\tLoss: 2.145602\n",
      "Train Epoch: 2 [6600/28547 (23%)]\tLoss: 1.965837\n",
      "Train Epoch: 2 [6900/28547 (24%)]\tLoss: 2.261594\n",
      "Train Epoch: 2 [7200/28547 (25%)]\tLoss: 2.099820\n",
      "Train Epoch: 2 [7500/28547 (26%)]\tLoss: 2.045378\n",
      "Train Epoch: 2 [7800/28547 (27%)]\tLoss: 2.134974\n",
      "Train Epoch: 2 [8100/28547 (28%)]\tLoss: 2.045108\n",
      "Train Epoch: 2 [8400/28547 (29%)]\tLoss: 2.051430\n",
      "Train Epoch: 2 [8700/28547 (30%)]\tLoss: 2.070955\n",
      "Train Epoch: 2 [9000/28547 (32%)]\tLoss: 2.008919\n",
      "Train Epoch: 2 [9300/28547 (33%)]\tLoss: 2.102368\n",
      "Train Epoch: 2 [9600/28547 (34%)]\tLoss: 2.140086\n",
      "Train Epoch: 2 [9900/28547 (35%)]\tLoss: 1.969826\n",
      "Train Epoch: 2 [10200/28547 (36%)]\tLoss: 1.998701\n",
      "Train Epoch: 2 [10500/28547 (37%)]\tLoss: 2.097779\n",
      "Train Epoch: 2 [10800/28547 (38%)]\tLoss: 1.986912\n",
      "Train Epoch: 2 [11100/28547 (39%)]\tLoss: 1.955679\n",
      "Train Epoch: 2 [11400/28547 (40%)]\tLoss: 2.137373\n",
      "Train Epoch: 2 [11700/28547 (41%)]\tLoss: 2.093325\n",
      "Train Epoch: 2 [12000/28547 (42%)]\tLoss: 2.004580\n",
      "Train Epoch: 2 [12300/28547 (43%)]\tLoss: 2.050539\n",
      "Train Epoch: 2 [12600/28547 (44%)]\tLoss: 2.156238\n",
      "Train Epoch: 2 [12900/28547 (45%)]\tLoss: 2.079184\n",
      "Train Epoch: 2 [13200/28547 (46%)]\tLoss: 2.160458\n",
      "Train Epoch: 2 [13500/28547 (47%)]\tLoss: 2.068335\n",
      "Train Epoch: 2 [13800/28547 (48%)]\tLoss: 1.998651\n",
      "Train Epoch: 2 [14100/28547 (49%)]\tLoss: 2.163374\n",
      "Train Epoch: 2 [14400/28547 (50%)]\tLoss: 2.266029\n",
      "Train Epoch: 2 [14700/28547 (51%)]\tLoss: 1.937814\n",
      "Train Epoch: 2 [15000/28547 (53%)]\tLoss: 2.086089\n",
      "Train Epoch: 2 [15300/28547 (54%)]\tLoss: 2.019573\n",
      "Train Epoch: 2 [15600/28547 (55%)]\tLoss: 2.178799\n",
      "Train Epoch: 2 [15900/28547 (56%)]\tLoss: 2.051936\n",
      "Train Epoch: 2 [16200/28547 (57%)]\tLoss: 2.083282\n",
      "Train Epoch: 2 [16500/28547 (58%)]\tLoss: 2.169994\n",
      "Train Epoch: 2 [16800/28547 (59%)]\tLoss: 2.171267\n",
      "Train Epoch: 2 [17100/28547 (60%)]\tLoss: 2.222086\n",
      "Train Epoch: 2 [17400/28547 (61%)]\tLoss: 2.026968\n",
      "Train Epoch: 2 [17700/28547 (62%)]\tLoss: 2.151555\n",
      "Train Epoch: 2 [18000/28547 (63%)]\tLoss: 2.067628\n",
      "Train Epoch: 2 [18300/28547 (64%)]\tLoss: 2.016672\n",
      "Train Epoch: 2 [18600/28547 (65%)]\tLoss: 2.081178\n",
      "Train Epoch: 2 [18900/28547 (66%)]\tLoss: 2.019505\n",
      "Train Epoch: 2 [19200/28547 (67%)]\tLoss: 2.164218\n",
      "Train Epoch: 2 [19500/28547 (68%)]\tLoss: 2.079611\n",
      "Train Epoch: 2 [19800/28547 (69%)]\tLoss: 2.164270\n",
      "Train Epoch: 2 [20100/28547 (70%)]\tLoss: 2.119389\n",
      "Train Epoch: 2 [20400/28547 (71%)]\tLoss: 2.112300\n",
      "Train Epoch: 2 [20700/28547 (73%)]\tLoss: 2.034013\n",
      "Train Epoch: 2 [21000/28547 (74%)]\tLoss: 2.129000\n",
      "Train Epoch: 2 [21300/28547 (75%)]\tLoss: 2.100658\n",
      "Train Epoch: 2 [21600/28547 (76%)]\tLoss: 2.106439\n",
      "Train Epoch: 2 [21900/28547 (77%)]\tLoss: 1.990808\n",
      "Train Epoch: 2 [22200/28547 (78%)]\tLoss: 2.099206\n",
      "Train Epoch: 2 [22500/28547 (79%)]\tLoss: 2.022888\n",
      "Train Epoch: 2 [22800/28547 (80%)]\tLoss: 2.093222\n",
      "Train Epoch: 2 [23100/28547 (81%)]\tLoss: 2.115648\n",
      "Train Epoch: 2 [23400/28547 (82%)]\tLoss: 1.993053\n",
      "Train Epoch: 2 [23700/28547 (83%)]\tLoss: 2.100096\n",
      "Train Epoch: 2 [24000/28547 (84%)]\tLoss: 2.045250\n",
      "Train Epoch: 2 [24300/28547 (85%)]\tLoss: 2.135376\n",
      "Train Epoch: 2 [24600/28547 (86%)]\tLoss: 2.046333\n",
      "Train Epoch: 2 [24900/28547 (87%)]\tLoss: 2.086328\n",
      "Train Epoch: 2 [25200/28547 (88%)]\tLoss: 2.158791\n",
      "Train Epoch: 2 [25500/28547 (89%)]\tLoss: 1.995624\n",
      "Train Epoch: 2 [25800/28547 (90%)]\tLoss: 2.132198\n",
      "Train Epoch: 2 [26100/28547 (91%)]\tLoss: 2.051986\n",
      "Train Epoch: 2 [26400/28547 (92%)]\tLoss: 2.007393\n",
      "Train Epoch: 2 [26700/28547 (94%)]\tLoss: 2.079824\n",
      "Train Epoch: 2 [27000/28547 (95%)]\tLoss: 2.089834\n",
      "Train Epoch: 2 [27300/28547 (96%)]\tLoss: 2.253170\n",
      "Train Epoch: 2 [27600/28547 (97%)]\tLoss: 2.073903\n",
      "Train Epoch: 2 [27900/28547 (98%)]\tLoss: 2.087127\n",
      "Train Epoch: 2 [28200/28547 (99%)]\tLoss: 2.059602\n",
      "Train Epoch: 2 [28500/28547 (100%)]\tLoss: 2.057553\n",
      "\n",
      "Evaluating…\n",
      "Test set: Average loss: 2.1574\n",
      "Average Greedy CER: 1.000000 Average Greedy WER: 1.0000\n",
      "\n",
      "Average Beam CER: 1.029631 Average Beam WER: 2.7043\n",
      "\n",
      "Train Epoch: 3 [0/28547 (0%)]\tLoss: 2.204685\n",
      "Train Epoch: 3 [300/28547 (1%)]\tLoss: 2.193092\n",
      "Train Epoch: 3 [600/28547 (2%)]\tLoss: 2.086378\n",
      "Train Epoch: 3 [900/28547 (3%)]\tLoss: 2.086339\n",
      "Train Epoch: 3 [1200/28547 (4%)]\tLoss: 2.023785\n",
      "Train Epoch: 3 [1500/28547 (5%)]\tLoss: 2.039726\n",
      "Train Epoch: 3 [1800/28547 (6%)]\tLoss: 2.003360\n",
      "Train Epoch: 3 [2100/28547 (7%)]\tLoss: 1.993860\n",
      "Train Epoch: 3 [2400/28547 (8%)]\tLoss: 2.031063\n",
      "Train Epoch: 3 [2700/28547 (9%)]\tLoss: 2.091388\n",
      "Train Epoch: 3 [3000/28547 (11%)]\tLoss: 1.958748\n",
      "Train Epoch: 3 [3300/28547 (12%)]\tLoss: 2.155007\n",
      "Train Epoch: 3 [3600/28547 (13%)]\tLoss: 2.084626\n",
      "Train Epoch: 3 [3900/28547 (14%)]\tLoss: 2.032501\n",
      "Train Epoch: 3 [4200/28547 (15%)]\tLoss: 2.007165\n",
      "Train Epoch: 3 [4500/28547 (16%)]\tLoss: 2.095678\n",
      "Train Epoch: 3 [4800/28547 (17%)]\tLoss: 2.085985\n",
      "Train Epoch: 3 [5100/28547 (18%)]\tLoss: 2.017306\n",
      "Train Epoch: 3 [5400/28547 (19%)]\tLoss: 2.113653\n",
      "Train Epoch: 3 [5700/28547 (20%)]\tLoss: 2.153008\n",
      "Train Epoch: 3 [6000/28547 (21%)]\tLoss: 2.081526\n",
      "Train Epoch: 3 [6300/28547 (22%)]\tLoss: 2.083391\n",
      "Train Epoch: 3 [6600/28547 (23%)]\tLoss: 2.032448\n",
      "Train Epoch: 3 [6900/28547 (24%)]\tLoss: 2.196814\n",
      "Train Epoch: 3 [7200/28547 (25%)]\tLoss: 2.086166\n",
      "Train Epoch: 3 [7500/28547 (26%)]\tLoss: 2.068175\n",
      "Train Epoch: 3 [7800/28547 (27%)]\tLoss: 2.094023\n",
      "Train Epoch: 3 [8100/28547 (28%)]\tLoss: 2.160238\n",
      "Train Epoch: 3 [8400/28547 (29%)]\tLoss: 2.079316\n",
      "Train Epoch: 3 [8700/28547 (30%)]\tLoss: 2.076055\n",
      "Train Epoch: 3 [9000/28547 (32%)]\tLoss: 2.173662\n",
      "Train Epoch: 3 [9300/28547 (33%)]\tLoss: 2.058887\n",
      "Train Epoch: 3 [9600/28547 (34%)]\tLoss: 2.097598\n",
      "Train Epoch: 3 [9900/28547 (35%)]\tLoss: 2.093843\n",
      "Train Epoch: 3 [10200/28547 (36%)]\tLoss: 2.136547\n",
      "Train Epoch: 3 [10500/28547 (37%)]\tLoss: 2.150348\n",
      "Train Epoch: 3 [10800/28547 (38%)]\tLoss: 2.068679\n",
      "Train Epoch: 3 [11100/28547 (39%)]\tLoss: 2.030064\n",
      "Train Epoch: 3 [11400/28547 (40%)]\tLoss: 2.133787\n",
      "Train Epoch: 3 [11700/28547 (41%)]\tLoss: 2.064728\n",
      "Train Epoch: 3 [12000/28547 (42%)]\tLoss: 2.086125\n",
      "Train Epoch: 3 [12300/28547 (43%)]\tLoss: 1.978089\n",
      "Train Epoch: 3 [12600/28547 (44%)]\tLoss: 2.037311\n",
      "Train Epoch: 3 [12900/28547 (45%)]\tLoss: 2.116357\n",
      "Train Epoch: 3 [13200/28547 (46%)]\tLoss: 2.033268\n",
      "Train Epoch: 3 [13500/28547 (47%)]\tLoss: 2.083038\n",
      "Train Epoch: 3 [13800/28547 (48%)]\tLoss: 2.056994\n",
      "Train Epoch: 3 [14100/28547 (49%)]\tLoss: 1.969607\n",
      "Train Epoch: 3 [14400/28547 (50%)]\tLoss: 2.056204\n",
      "Train Epoch: 3 [14700/28547 (51%)]\tLoss: 2.083894\n",
      "Train Epoch: 3 [15000/28547 (53%)]\tLoss: 1.953411\n",
      "Train Epoch: 3 [15300/28547 (54%)]\tLoss: 2.115666\n",
      "Train Epoch: 3 [15600/28547 (55%)]\tLoss: 2.233764\n",
      "Train Epoch: 3 [15900/28547 (56%)]\tLoss: 2.135613\n",
      "Train Epoch: 3 [16200/28547 (57%)]\tLoss: 2.220514\n",
      "Train Epoch: 3 [16500/28547 (58%)]\tLoss: 2.192832\n",
      "Train Epoch: 3 [16800/28547 (59%)]\tLoss: 2.125214\n",
      "Train Epoch: 3 [17100/28547 (60%)]\tLoss: 2.054239\n",
      "Train Epoch: 3 [17400/28547 (61%)]\tLoss: 2.250147\n",
      "Train Epoch: 3 [17700/28547 (62%)]\tLoss: 2.198890\n",
      "Train Epoch: 3 [18000/28547 (63%)]\tLoss: 2.133855\n",
      "Train Epoch: 3 [18300/28547 (64%)]\tLoss: 2.141043\n",
      "Train Epoch: 3 [18600/28547 (65%)]\tLoss: 2.093890\n",
      "Train Epoch: 3 [18900/28547 (66%)]\tLoss: 2.005040\n",
      "Train Epoch: 3 [19200/28547 (67%)]\tLoss: 2.035047\n",
      "Train Epoch: 3 [19500/28547 (68%)]\tLoss: 2.086148\n",
      "Train Epoch: 3 [19800/28547 (69%)]\tLoss: 2.004854\n",
      "Train Epoch: 3 [20100/28547 (70%)]\tLoss: 2.152883\n",
      "Train Epoch: 3 [20400/28547 (71%)]\tLoss: 2.057334\n",
      "Train Epoch: 3 [20700/28547 (73%)]\tLoss: 2.138748\n",
      "Train Epoch: 3 [21000/28547 (74%)]\tLoss: 2.174191\n",
      "Train Epoch: 3 [21300/28547 (75%)]\tLoss: 2.163782\n",
      "Train Epoch: 3 [21600/28547 (76%)]\tLoss: 2.170314\n",
      "Train Epoch: 3 [21900/28547 (77%)]\tLoss: 2.078702\n",
      "Train Epoch: 3 [22200/28547 (78%)]\tLoss: 2.086957\n",
      "Train Epoch: 3 [22500/28547 (79%)]\tLoss: 2.090816\n",
      "Train Epoch: 3 [22800/28547 (80%)]\tLoss: 2.161408\n",
      "Train Epoch: 3 [23100/28547 (81%)]\tLoss: 2.176270\n",
      "Train Epoch: 3 [23400/28547 (82%)]\tLoss: 1.971400\n",
      "Train Epoch: 3 [23700/28547 (83%)]\tLoss: 2.057332\n",
      "Train Epoch: 3 [24000/28547 (84%)]\tLoss: 2.070130\n",
      "Train Epoch: 3 [24300/28547 (85%)]\tLoss: 2.153406\n",
      "Train Epoch: 3 [24600/28547 (86%)]\tLoss: 2.086281\n",
      "Train Epoch: 3 [24900/28547 (87%)]\tLoss: 2.084558\n",
      "Train Epoch: 3 [25200/28547 (88%)]\tLoss: 2.102292\n",
      "Train Epoch: 3 [25500/28547 (89%)]\tLoss: 2.159809\n",
      "Train Epoch: 3 [25800/28547 (90%)]\tLoss: 2.086807\n",
      "Train Epoch: 3 [26100/28547 (91%)]\tLoss: 2.025297\n",
      "Train Epoch: 3 [26400/28547 (92%)]\tLoss: 2.161644\n",
      "Train Epoch: 3 [26700/28547 (94%)]\tLoss: 2.122809\n",
      "Train Epoch: 3 [27000/28547 (95%)]\tLoss: 2.081383\n",
      "Train Epoch: 3 [27300/28547 (96%)]\tLoss: 2.150156\n",
      "Train Epoch: 3 [27600/28547 (97%)]\tLoss: 2.114967\n",
      "Train Epoch: 3 [27900/28547 (98%)]\tLoss: 2.159699\n",
      "Train Epoch: 3 [28200/28547 (99%)]\tLoss: 2.138756\n",
      "Train Epoch: 3 [28500/28547 (100%)]\tLoss: 2.082359\n",
      "\n",
      "Evaluating…\n",
      "Test set: Average loss: 2.1549\n",
      "Average Greedy CER: 1.000000 Average Greedy WER: 1.0000\n",
      "\n",
      "Average Beam CER: 1.093814 Average Beam WER: 2.9845\n",
      "\n",
      "Train Epoch: 4 [0/28547 (0%)]\tLoss: 2.053037\n",
      "Train Epoch: 4 [300/28547 (1%)]\tLoss: 2.091095\n",
      "Train Epoch: 4 [600/28547 (2%)]\tLoss: 2.102129\n",
      "Train Epoch: 4 [900/28547 (3%)]\tLoss: 2.010523\n",
      "Train Epoch: 4 [1200/28547 (4%)]\tLoss: 2.204911\n",
      "Train Epoch: 4 [1500/28547 (5%)]\tLoss: 2.110674\n",
      "Train Epoch: 4 [1800/28547 (6%)]\tLoss: 2.134809\n",
      "Train Epoch: 4 [2100/28547 (7%)]\tLoss: 2.081294\n",
      "Train Epoch: 4 [2400/28547 (8%)]\tLoss: 2.060407\n",
      "Train Epoch: 4 [2700/28547 (9%)]\tLoss: 1.974761\n",
      "Train Epoch: 4 [3000/28547 (11%)]\tLoss: 2.178288\n",
      "Train Epoch: 4 [3300/28547 (12%)]\tLoss: 2.020849\n",
      "Train Epoch: 4 [3600/28547 (13%)]\tLoss: 2.094257\n",
      "Train Epoch: 4 [3900/28547 (14%)]\tLoss: 2.123121\n",
      "Train Epoch: 4 [4200/28547 (15%)]\tLoss: 2.287715\n",
      "Train Epoch: 4 [4500/28547 (16%)]\tLoss: 2.082573\n",
      "Train Epoch: 4 [4800/28547 (17%)]\tLoss: 2.065472\n",
      "Train Epoch: 4 [5100/28547 (18%)]\tLoss: 1.959999\n",
      "Train Epoch: 4 [5400/28547 (19%)]\tLoss: 2.037475\n",
      "Train Epoch: 4 [5700/28547 (20%)]\tLoss: 2.053635\n",
      "Train Epoch: 4 [6000/28547 (21%)]\tLoss: 2.113652\n",
      "Train Epoch: 4 [6300/28547 (22%)]\tLoss: 2.069969\n",
      "Train Epoch: 4 [6600/28547 (23%)]\tLoss: 2.040007\n",
      "Train Epoch: 4 [6900/28547 (24%)]\tLoss: 2.172691\n",
      "Train Epoch: 4 [7200/28547 (25%)]\tLoss: 2.029235\n",
      "Train Epoch: 4 [7500/28547 (26%)]\tLoss: 2.010515\n",
      "Train Epoch: 4 [7800/28547 (27%)]\tLoss: 2.179394\n",
      "Train Epoch: 4 [8100/28547 (28%)]\tLoss: 2.259573\n",
      "Train Epoch: 4 [8400/28547 (29%)]\tLoss: 2.128113\n",
      "Train Epoch: 4 [8700/28547 (30%)]\tLoss: 2.001660\n",
      "Train Epoch: 4 [9000/28547 (32%)]\tLoss: 1.921033\n",
      "Train Epoch: 4 [9300/28547 (33%)]\tLoss: 2.033882\n",
      "Train Epoch: 4 [9600/28547 (34%)]\tLoss: 2.162795\n",
      "Train Epoch: 4 [9900/28547 (35%)]\tLoss: 1.956024\n",
      "Train Epoch: 4 [10200/28547 (36%)]\tLoss: 2.167564\n",
      "Train Epoch: 4 [10500/28547 (37%)]\tLoss: 2.091376\n",
      "Train Epoch: 4 [10800/28547 (38%)]\tLoss: 2.121424\n",
      "Train Epoch: 4 [11100/28547 (39%)]\tLoss: 2.183674\n",
      "Train Epoch: 4 [11400/28547 (40%)]\tLoss: 1.972873\n",
      "Train Epoch: 4 [11700/28547 (41%)]\tLoss: 2.084147\n",
      "Train Epoch: 4 [12000/28547 (42%)]\tLoss: 2.160751\n",
      "Train Epoch: 4 [12300/28547 (43%)]\tLoss: 2.192372\n",
      "Train Epoch: 4 [12600/28547 (44%)]\tLoss: 2.082050\n",
      "Train Epoch: 4 [12900/28547 (45%)]\tLoss: 2.116229\n",
      "Train Epoch: 4 [13200/28547 (46%)]\tLoss: 2.053314\n",
      "Train Epoch: 4 [13500/28547 (47%)]\tLoss: 2.126368\n",
      "Train Epoch: 4 [13800/28547 (48%)]\tLoss: 2.090178\n",
      "Train Epoch: 4 [14100/28547 (49%)]\tLoss: 2.187235\n",
      "Train Epoch: 4 [14400/28547 (50%)]\tLoss: 2.127173\n",
      "Train Epoch: 4 [14700/28547 (51%)]\tLoss: 2.127182\n",
      "Train Epoch: 4 [15000/28547 (53%)]\tLoss: 2.156180\n",
      "Train Epoch: 4 [15300/28547 (54%)]\tLoss: 2.113780\n",
      "Train Epoch: 4 [15600/28547 (55%)]\tLoss: 2.162207\n",
      "Train Epoch: 4 [15900/28547 (56%)]\tLoss: 1.996819\n",
      "Train Epoch: 4 [16200/28547 (57%)]\tLoss: 2.115906\n",
      "Train Epoch: 4 [16500/28547 (58%)]\tLoss: 2.077258\n",
      "Train Epoch: 4 [16800/28547 (59%)]\tLoss: 2.110390\n",
      "Train Epoch: 4 [17100/28547 (60%)]\tLoss: 2.089585\n",
      "Train Epoch: 4 [17400/28547 (61%)]\tLoss: 2.123332\n",
      "Train Epoch: 4 [17700/28547 (62%)]\tLoss: 2.166292\n",
      "Train Epoch: 4 [18000/28547 (63%)]\tLoss: 2.101396\n",
      "Train Epoch: 4 [18300/28547 (64%)]\tLoss: 2.070464\n",
      "Train Epoch: 4 [18600/28547 (65%)]\tLoss: 2.142359\n",
      "Train Epoch: 4 [18900/28547 (66%)]\tLoss: 2.090492\n",
      "Train Epoch: 4 [19200/28547 (67%)]\tLoss: 2.189001\n",
      "Train Epoch: 4 [19500/28547 (68%)]\tLoss: 2.288274\n",
      "Train Epoch: 4 [19800/28547 (69%)]\tLoss: 2.105598\n",
      "Train Epoch: 4 [20100/28547 (70%)]\tLoss: 2.022876\n",
      "Train Epoch: 4 [20400/28547 (71%)]\tLoss: 2.105627\n",
      "Train Epoch: 4 [20700/28547 (73%)]\tLoss: 2.090069\n",
      "Train Epoch: 4 [21000/28547 (74%)]\tLoss: 2.076956\n",
      "Train Epoch: 4 [21300/28547 (75%)]\tLoss: 2.095426\n",
      "Train Epoch: 4 [21600/28547 (76%)]\tLoss: 2.057412\n",
      "Train Epoch: 4 [21900/28547 (77%)]\tLoss: 1.974148\n",
      "Train Epoch: 4 [22200/28547 (78%)]\tLoss: 1.985316\n",
      "Train Epoch: 4 [22500/28547 (79%)]\tLoss: 2.089002\n",
      "Train Epoch: 4 [22800/28547 (80%)]\tLoss: 1.912126\n",
      "Train Epoch: 4 [23100/28547 (81%)]\tLoss: 2.160577\n",
      "Train Epoch: 4 [23400/28547 (82%)]\tLoss: 1.985588\n",
      "Train Epoch: 4 [23700/28547 (83%)]\tLoss: 2.060765\n",
      "Train Epoch: 4 [24000/28547 (84%)]\tLoss: 2.103045\n",
      "Train Epoch: 4 [24300/28547 (85%)]\tLoss: 2.110717\n",
      "Train Epoch: 4 [24600/28547 (86%)]\tLoss: 2.109582\n",
      "Train Epoch: 4 [24900/28547 (87%)]\tLoss: 2.156971\n",
      "Train Epoch: 4 [25200/28547 (88%)]\tLoss: 2.117768\n",
      "Train Epoch: 4 [25500/28547 (89%)]\tLoss: 2.122843\n",
      "Train Epoch: 4 [25800/28547 (90%)]\tLoss: 1.929651\n",
      "Train Epoch: 4 [26100/28547 (91%)]\tLoss: 2.124089\n",
      "Train Epoch: 4 [26400/28547 (92%)]\tLoss: 2.196990\n",
      "Train Epoch: 4 [26700/28547 (94%)]\tLoss: 2.130681\n",
      "Train Epoch: 4 [27000/28547 (95%)]\tLoss: 2.163775\n",
      "Train Epoch: 4 [27300/28547 (96%)]\tLoss: 2.072125\n",
      "Train Epoch: 4 [27600/28547 (97%)]\tLoss: 2.085396\n",
      "Train Epoch: 4 [27900/28547 (98%)]\tLoss: 2.280955\n",
      "Train Epoch: 4 [28200/28547 (99%)]\tLoss: 2.271424\n",
      "Train Epoch: 4 [28500/28547 (100%)]\tLoss: 2.066260\n",
      "\n",
      "Evaluating…\n",
      "Test set: Average loss: 2.1593\n",
      "Average Greedy CER: 1.000000 Average Greedy WER: 1.0000\n",
      "\n",
      "Average Beam CER: 1.186884 Average Beam WER: 3.3597\n",
      "\n",
      "Train Epoch: 5 [0/28547 (0%)]\tLoss: 2.164864\n",
      "Train Epoch: 5 [300/28547 (1%)]\tLoss: 2.111594\n",
      "Train Epoch: 5 [600/28547 (2%)]\tLoss: 2.050516\n",
      "Train Epoch: 5 [900/28547 (3%)]\tLoss: 2.063070\n",
      "Train Epoch: 5 [1200/28547 (4%)]\tLoss: 2.092891\n",
      "Train Epoch: 5 [1500/28547 (5%)]\tLoss: 2.028366\n",
      "Train Epoch: 5 [1800/28547 (6%)]\tLoss: 2.068856\n",
      "Train Epoch: 5 [2100/28547 (7%)]\tLoss: 2.139385\n",
      "Train Epoch: 5 [2400/28547 (8%)]\tLoss: 2.136589\n",
      "Train Epoch: 5 [2700/28547 (9%)]\tLoss: 2.109518\n",
      "Train Epoch: 5 [3000/28547 (11%)]\tLoss: 2.136739\n",
      "Train Epoch: 5 [3300/28547 (12%)]\tLoss: 2.069899\n",
      "Train Epoch: 5 [3600/28547 (13%)]\tLoss: 2.123240\n",
      "Train Epoch: 5 [3900/28547 (14%)]\tLoss: 1.929907\n",
      "Train Epoch: 5 [4200/28547 (15%)]\tLoss: 2.047501\n",
      "Train Epoch: 5 [4500/28547 (16%)]\tLoss: 2.109162\n",
      "Train Epoch: 5 [4800/28547 (17%)]\tLoss: 2.086054\n",
      "Train Epoch: 5 [5100/28547 (18%)]\tLoss: 2.103164\n",
      "Train Epoch: 5 [5400/28547 (19%)]\tLoss: 2.053449\n",
      "Train Epoch: 5 [5700/28547 (20%)]\tLoss: 2.125002\n",
      "Train Epoch: 5 [6000/28547 (21%)]\tLoss: 2.065256\n",
      "Train Epoch: 5 [6300/28547 (22%)]\tLoss: 2.189289\n",
      "Train Epoch: 5 [6600/28547 (23%)]\tLoss: 2.144974\n",
      "Train Epoch: 5 [6900/28547 (24%)]\tLoss: 2.018705\n",
      "Train Epoch: 5 [7200/28547 (25%)]\tLoss: 2.167887\n",
      "Train Epoch: 5 [7500/28547 (26%)]\tLoss: 2.138832\n",
      "Train Epoch: 5 [7800/28547 (27%)]\tLoss: 2.024382\n",
      "Train Epoch: 5 [8100/28547 (28%)]\tLoss: 2.067895\n",
      "Train Epoch: 5 [8400/28547 (29%)]\tLoss: 2.093280\n",
      "Train Epoch: 5 [8700/28547 (30%)]\tLoss: 2.048199\n",
      "Train Epoch: 5 [9000/28547 (32%)]\tLoss: 2.199746\n",
      "Train Epoch: 5 [9300/28547 (33%)]\tLoss: 2.066117\n",
      "Train Epoch: 5 [9600/28547 (34%)]\tLoss: 2.110726\n",
      "Train Epoch: 5 [9900/28547 (35%)]\tLoss: 2.177418\n",
      "Train Epoch: 5 [10200/28547 (36%)]\tLoss: 2.102962\n",
      "Train Epoch: 5 [10500/28547 (37%)]\tLoss: 2.113459\n",
      "Train Epoch: 5 [10800/28547 (38%)]\tLoss: 2.220132\n",
      "Train Epoch: 5 [11100/28547 (39%)]\tLoss: 2.129642\n",
      "Train Epoch: 5 [11400/28547 (40%)]\tLoss: 2.037447\n",
      "Train Epoch: 5 [11700/28547 (41%)]\tLoss: 2.026093\n",
      "Train Epoch: 5 [12000/28547 (42%)]\tLoss: 2.095180\n",
      "Train Epoch: 5 [12300/28547 (43%)]\tLoss: 2.098589\n",
      "Train Epoch: 5 [12600/28547 (44%)]\tLoss: 2.146641\n",
      "Train Epoch: 5 [12900/28547 (45%)]\tLoss: 2.287114\n",
      "Train Epoch: 5 [13200/28547 (46%)]\tLoss: 2.144830\n",
      "Train Epoch: 5 [13500/28547 (47%)]\tLoss: 2.154868\n",
      "Train Epoch: 5 [13800/28547 (48%)]\tLoss: 2.092940\n",
      "Train Epoch: 5 [14100/28547 (49%)]\tLoss: 2.039298\n",
      "Train Epoch: 5 [14400/28547 (50%)]\tLoss: 2.168060\n",
      "Train Epoch: 5 [14700/28547 (51%)]\tLoss: 2.090998\n",
      "Train Epoch: 5 [15000/28547 (53%)]\tLoss: 2.019660\n",
      "Train Epoch: 5 [15300/28547 (54%)]\tLoss: 1.979701\n",
      "Train Epoch: 5 [15600/28547 (55%)]\tLoss: 2.055513\n",
      "Train Epoch: 5 [15900/28547 (56%)]\tLoss: 2.135691\n",
      "Train Epoch: 5 [16200/28547 (57%)]\tLoss: 2.160321\n",
      "Train Epoch: 5 [16500/28547 (58%)]\tLoss: 2.137107\n",
      "Train Epoch: 5 [16800/28547 (59%)]\tLoss: 2.005762\n",
      "Train Epoch: 5 [17100/28547 (60%)]\tLoss: 2.074400\n",
      "Train Epoch: 5 [17400/28547 (61%)]\tLoss: 2.230917\n",
      "Train Epoch: 5 [17700/28547 (62%)]\tLoss: 2.002275\n",
      "Train Epoch: 5 [18000/28547 (63%)]\tLoss: 2.072008\n",
      "Train Epoch: 5 [18300/28547 (64%)]\tLoss: 2.085977\n",
      "Train Epoch: 5 [18600/28547 (65%)]\tLoss: 1.994505\n",
      "Train Epoch: 5 [18900/28547 (66%)]\tLoss: 2.005481\n",
      "Train Epoch: 5 [19200/28547 (67%)]\tLoss: 2.127553\n",
      "Train Epoch: 5 [19500/28547 (68%)]\tLoss: 2.136850\n",
      "Train Epoch: 5 [19800/28547 (69%)]\tLoss: 2.049776\n",
      "Train Epoch: 5 [20100/28547 (70%)]\tLoss: 2.114211\n",
      "Train Epoch: 5 [20400/28547 (71%)]\tLoss: 2.025966\n",
      "Train Epoch: 5 [20700/28547 (73%)]\tLoss: 2.161206\n",
      "Train Epoch: 5 [21000/28547 (74%)]\tLoss: 2.033770\n",
      "Train Epoch: 5 [21300/28547 (75%)]\tLoss: 2.064280\n",
      "Train Epoch: 5 [21600/28547 (76%)]\tLoss: 2.092938\n",
      "Train Epoch: 5 [21900/28547 (77%)]\tLoss: 2.068066\n",
      "Train Epoch: 5 [22200/28547 (78%)]\tLoss: 1.953511\n",
      "Train Epoch: 5 [22500/28547 (79%)]\tLoss: 2.134219\n",
      "Train Epoch: 5 [22800/28547 (80%)]\tLoss: 2.143058\n",
      "Train Epoch: 5 [23100/28547 (81%)]\tLoss: 2.010219\n",
      "Train Epoch: 5 [23400/28547 (82%)]\tLoss: 2.191590\n",
      "Train Epoch: 5 [23700/28547 (83%)]\tLoss: 2.102393\n",
      "Train Epoch: 5 [24000/28547 (84%)]\tLoss: 2.113271\n",
      "Train Epoch: 5 [24300/28547 (85%)]\tLoss: 2.166928\n",
      "Train Epoch: 5 [24600/28547 (86%)]\tLoss: 2.038865\n",
      "Train Epoch: 5 [24900/28547 (87%)]\tLoss: 1.951669\n",
      "Train Epoch: 5 [25200/28547 (88%)]\tLoss: 2.159228\n",
      "Train Epoch: 5 [25500/28547 (89%)]\tLoss: 2.088572\n",
      "Train Epoch: 5 [25800/28547 (90%)]\tLoss: 2.073641\n",
      "Train Epoch: 5 [26100/28547 (91%)]\tLoss: 2.257204\n",
      "Train Epoch: 5 [26400/28547 (92%)]\tLoss: 2.069091\n",
      "Train Epoch: 5 [26700/28547 (94%)]\tLoss: 2.141947\n",
      "Train Epoch: 5 [27000/28547 (95%)]\tLoss: 2.229862\n",
      "Train Epoch: 5 [27300/28547 (96%)]\tLoss: 2.019683\n",
      "Train Epoch: 5 [27600/28547 (97%)]\tLoss: 2.168186\n",
      "Train Epoch: 5 [27900/28547 (98%)]\tLoss: 2.006461\n",
      "Train Epoch: 5 [28200/28547 (99%)]\tLoss: 2.036828\n",
      "Train Epoch: 5 [28500/28547 (100%)]\tLoss: 2.133138\n",
      "\n",
      "Evaluating…\n",
      "Test set: Average loss: 2.1541\n",
      "Average Greedy CER: 1.000000 Average Greedy WER: 1.0000\n",
      "\n",
      "Average Beam CER: 1.109424 Average Beam WER: 3.0495\n",
      "\n",
      "Train Epoch: 6 [0/28547 (0%)]\tLoss: 2.132421\n",
      "Train Epoch: 6 [300/28547 (1%)]\tLoss: 1.997443\n",
      "Train Epoch: 6 [600/28547 (2%)]\tLoss: 2.122207\n",
      "Train Epoch: 6 [900/28547 (3%)]\tLoss: 2.166049\n",
      "Train Epoch: 6 [1200/28547 (4%)]\tLoss: 2.023894\n",
      "Train Epoch: 6 [1500/28547 (5%)]\tLoss: 2.323841\n",
      "Train Epoch: 6 [1800/28547 (6%)]\tLoss: 2.066867\n",
      "Train Epoch: 6 [2100/28547 (7%)]\tLoss: 2.207871\n",
      "Train Epoch: 6 [2400/28547 (8%)]\tLoss: 2.125660\n",
      "Train Epoch: 6 [2700/28547 (9%)]\tLoss: 2.177948\n",
      "Train Epoch: 6 [3000/28547 (11%)]\tLoss: 2.105851\n",
      "Train Epoch: 6 [3300/28547 (12%)]\tLoss: 2.107341\n",
      "Train Epoch: 6 [3600/28547 (13%)]\tLoss: 2.051785\n",
      "Train Epoch: 6 [3900/28547 (14%)]\tLoss: 2.126624\n",
      "Train Epoch: 6 [4200/28547 (15%)]\tLoss: 2.153797\n",
      "Train Epoch: 6 [4500/28547 (16%)]\tLoss: 2.156737\n",
      "Train Epoch: 6 [4800/28547 (17%)]\tLoss: 2.055106\n",
      "Train Epoch: 6 [5100/28547 (18%)]\tLoss: 2.035219\n",
      "Train Epoch: 6 [5400/28547 (19%)]\tLoss: 2.238298\n",
      "Train Epoch: 6 [5700/28547 (20%)]\tLoss: 2.024479\n",
      "Train Epoch: 6 [6000/28547 (21%)]\tLoss: 2.149809\n",
      "Train Epoch: 6 [6300/28547 (22%)]\tLoss: 2.094147\n",
      "Train Epoch: 6 [6600/28547 (23%)]\tLoss: 2.179345\n",
      "Train Epoch: 6 [6900/28547 (24%)]\tLoss: 2.088162\n",
      "Train Epoch: 6 [7200/28547 (25%)]\tLoss: 2.076006\n",
      "Train Epoch: 6 [7500/28547 (26%)]\tLoss: 2.018029\n",
      "Train Epoch: 6 [7800/28547 (27%)]\tLoss: 2.115197\n",
      "Train Epoch: 6 [8100/28547 (28%)]\tLoss: 2.024473\n",
      "Train Epoch: 6 [8400/28547 (29%)]\tLoss: 2.090116\n",
      "Train Epoch: 6 [8700/28547 (30%)]\tLoss: 1.990797\n",
      "Train Epoch: 6 [9000/28547 (32%)]\tLoss: 2.173053\n",
      "Train Epoch: 6 [9300/28547 (33%)]\tLoss: 2.155099\n",
      "Train Epoch: 6 [9600/28547 (34%)]\tLoss: 2.117195\n",
      "Train Epoch: 6 [9900/28547 (35%)]\tLoss: 2.143111\n",
      "Train Epoch: 6 [10200/28547 (36%)]\tLoss: 2.113793\n",
      "Train Epoch: 6 [10500/28547 (37%)]\tLoss: 2.204201\n",
      "Train Epoch: 6 [10800/28547 (38%)]\tLoss: 2.149341\n",
      "Train Epoch: 6 [11100/28547 (39%)]\tLoss: 2.171734\n",
      "Train Epoch: 6 [11400/28547 (40%)]\tLoss: 2.077429\n",
      "Train Epoch: 6 [11700/28547 (41%)]\tLoss: 2.250544\n",
      "Train Epoch: 6 [12000/28547 (42%)]\tLoss: 2.088198\n",
      "Train Epoch: 6 [12300/28547 (43%)]\tLoss: 2.046324\n",
      "Train Epoch: 6 [12600/28547 (44%)]\tLoss: 2.072244\n",
      "Train Epoch: 6 [12900/28547 (45%)]\tLoss: 2.096410\n",
      "Train Epoch: 6 [13200/28547 (46%)]\tLoss: 2.163684\n",
      "Train Epoch: 6 [13500/28547 (47%)]\tLoss: 2.022006\n",
      "Train Epoch: 6 [13800/28547 (48%)]\tLoss: 2.035983\n",
      "Train Epoch: 6 [14100/28547 (49%)]\tLoss: 2.103016\n",
      "Train Epoch: 6 [14400/28547 (50%)]\tLoss: 1.941466\n",
      "Train Epoch: 6 [14700/28547 (51%)]\tLoss: 2.207099\n",
      "Train Epoch: 6 [15000/28547 (53%)]\tLoss: 2.216209\n",
      "Train Epoch: 6 [15300/28547 (54%)]\tLoss: 2.050932\n",
      "Train Epoch: 6 [15600/28547 (55%)]\tLoss: 2.088246\n",
      "Train Epoch: 6 [15900/28547 (56%)]\tLoss: 2.170519\n",
      "Train Epoch: 6 [16200/28547 (57%)]\tLoss: 2.113238\n",
      "Train Epoch: 6 [16500/28547 (58%)]\tLoss: 2.108670\n",
      "Train Epoch: 6 [16800/28547 (59%)]\tLoss: 2.087208\n",
      "Train Epoch: 6 [17100/28547 (60%)]\tLoss: 2.089660\n",
      "Train Epoch: 6 [17400/28547 (61%)]\tLoss: 2.032245\n",
      "Train Epoch: 6 [17700/28547 (62%)]\tLoss: 2.122240\n",
      "Train Epoch: 6 [18000/28547 (63%)]\tLoss: 2.087360\n",
      "Train Epoch: 6 [18300/28547 (64%)]\tLoss: 2.094724\n",
      "Train Epoch: 6 [18600/28547 (65%)]\tLoss: 2.132120\n",
      "Train Epoch: 6 [18900/28547 (66%)]\tLoss: 2.089352\n",
      "Train Epoch: 6 [19200/28547 (67%)]\tLoss: 2.124825\n",
      "Train Epoch: 6 [19500/28547 (68%)]\tLoss: 2.140880\n",
      "Train Epoch: 6 [19800/28547 (69%)]\tLoss: 2.044533\n",
      "Train Epoch: 6 [20100/28547 (70%)]\tLoss: 1.982602\n",
      "Train Epoch: 6 [20400/28547 (71%)]\tLoss: 2.054790\n",
      "Train Epoch: 6 [20700/28547 (73%)]\tLoss: 2.129595\n",
      "Train Epoch: 6 [21000/28547 (74%)]\tLoss: 2.047397\n",
      "Train Epoch: 6 [21300/28547 (75%)]\tLoss: 2.128065\n",
      "Train Epoch: 6 [21600/28547 (76%)]\tLoss: 2.039875\n",
      "Train Epoch: 6 [21900/28547 (77%)]\tLoss: 2.130136\n",
      "Train Epoch: 6 [22200/28547 (78%)]\tLoss: 2.025207\n",
      "Train Epoch: 6 [22500/28547 (79%)]\tLoss: 2.175182\n",
      "Train Epoch: 6 [22800/28547 (80%)]\tLoss: 2.191374\n",
      "Train Epoch: 6 [23100/28547 (81%)]\tLoss: 2.119252\n",
      "Train Epoch: 6 [23400/28547 (82%)]\tLoss: 2.038629\n",
      "Train Epoch: 6 [23700/28547 (83%)]\tLoss: 1.973829\n",
      "Train Epoch: 6 [24000/28547 (84%)]\tLoss: 2.197435\n",
      "Train Epoch: 6 [24300/28547 (85%)]\tLoss: 2.049841\n",
      "Train Epoch: 6 [24600/28547 (86%)]\tLoss: 2.058516\n",
      "Train Epoch: 6 [24900/28547 (87%)]\tLoss: 2.133933\n",
      "Train Epoch: 6 [25200/28547 (88%)]\tLoss: 1.969200\n",
      "Train Epoch: 6 [25500/28547 (89%)]\tLoss: 2.131087\n",
      "Train Epoch: 6 [25800/28547 (90%)]\tLoss: 2.045041\n",
      "Train Epoch: 6 [26100/28547 (91%)]\tLoss: 2.014723\n",
      "Train Epoch: 6 [26400/28547 (92%)]\tLoss: 2.031801\n",
      "Train Epoch: 6 [26700/28547 (94%)]\tLoss: 2.125071\n",
      "Train Epoch: 6 [27000/28547 (95%)]\tLoss: 2.073746\n",
      "Train Epoch: 6 [27300/28547 (96%)]\tLoss: 2.054626\n",
      "Train Epoch: 6 [27600/28547 (97%)]\tLoss: 2.110325\n",
      "Train Epoch: 6 [27900/28547 (98%)]\tLoss: 2.006594\n",
      "Train Epoch: 6 [28200/28547 (99%)]\tLoss: 2.256876\n",
      "Train Epoch: 6 [28500/28547 (100%)]\tLoss: 2.114123\n",
      "\n",
      "Evaluating…\n",
      "Test set: Average loss: 2.1543\n",
      "Average Greedy CER: 1.000000 Average Greedy WER: 1.0000\n",
      "\n",
      "Average Beam CER: 1.130857 Average Beam WER: 3.1386\n",
      "\n",
      "Train Epoch: 7 [0/28547 (0%)]\tLoss: 2.040704\n",
      "Train Epoch: 7 [300/28547 (1%)]\tLoss: 2.059646\n",
      "Train Epoch: 7 [600/28547 (2%)]\tLoss: 1.941646\n",
      "Train Epoch: 7 [900/28547 (3%)]\tLoss: 2.133753\n",
      "Train Epoch: 7 [1200/28547 (4%)]\tLoss: 2.040408\n",
      "Train Epoch: 7 [1500/28547 (5%)]\tLoss: 2.125041\n",
      "Train Epoch: 7 [1800/28547 (6%)]\tLoss: 2.107335\n",
      "Train Epoch: 7 [2100/28547 (7%)]\tLoss: 2.100506\n",
      "Train Epoch: 7 [2400/28547 (8%)]\tLoss: 2.048887\n",
      "Train Epoch: 7 [2700/28547 (9%)]\tLoss: 2.106004\n",
      "Train Epoch: 7 [3000/28547 (11%)]\tLoss: 2.086389\n",
      "Train Epoch: 7 [3300/28547 (12%)]\tLoss: 2.122172\n",
      "Train Epoch: 7 [3600/28547 (13%)]\tLoss: 2.097617\n",
      "Train Epoch: 7 [3900/28547 (14%)]\tLoss: 2.071310\n",
      "Train Epoch: 7 [4200/28547 (15%)]\tLoss: 2.032985\n",
      "Train Epoch: 7 [4500/28547 (16%)]\tLoss: 2.159676\n",
      "Train Epoch: 7 [4800/28547 (17%)]\tLoss: 2.123299\n",
      "Train Epoch: 7 [5100/28547 (18%)]\tLoss: 2.122979\n",
      "Train Epoch: 7 [5400/28547 (19%)]\tLoss: 2.142868\n",
      "Train Epoch: 7 [5700/28547 (20%)]\tLoss: 2.155537\n",
      "Train Epoch: 7 [6000/28547 (21%)]\tLoss: 2.067919\n",
      "Train Epoch: 7 [6300/28547 (22%)]\tLoss: 2.016411\n",
      "Train Epoch: 7 [6600/28547 (23%)]\tLoss: 2.029893\n",
      "Train Epoch: 7 [6900/28547 (24%)]\tLoss: 2.203372\n",
      "Train Epoch: 7 [7200/28547 (25%)]\tLoss: 2.082018\n",
      "Train Epoch: 7 [7500/28547 (26%)]\tLoss: 2.117180\n",
      "Train Epoch: 7 [7800/28547 (27%)]\tLoss: 1.975095\n",
      "Train Epoch: 7 [8100/28547 (28%)]\tLoss: 2.095667\n",
      "Train Epoch: 7 [8400/28547 (29%)]\tLoss: 2.111713\n",
      "Train Epoch: 7 [8700/28547 (30%)]\tLoss: 2.057627\n",
      "Train Epoch: 7 [9000/28547 (32%)]\tLoss: 2.126699\n",
      "Train Epoch: 7 [9300/28547 (33%)]\tLoss: 2.133616\n",
      "Train Epoch: 7 [9600/28547 (34%)]\tLoss: 2.058664\n",
      "Train Epoch: 7 [9900/28547 (35%)]\tLoss: 2.104399\n",
      "Train Epoch: 7 [10200/28547 (36%)]\tLoss: 2.011449\n",
      "Train Epoch: 7 [10500/28547 (37%)]\tLoss: 1.965297\n",
      "Train Epoch: 7 [10800/28547 (38%)]\tLoss: 2.108510\n",
      "Train Epoch: 7 [11100/28547 (39%)]\tLoss: 2.057698\n",
      "Train Epoch: 7 [11400/28547 (40%)]\tLoss: 2.072608\n",
      "Train Epoch: 7 [11700/28547 (41%)]\tLoss: 2.145358\n",
      "Train Epoch: 7 [12000/28547 (42%)]\tLoss: 2.106537\n",
      "Train Epoch: 7 [12300/28547 (43%)]\tLoss: 2.207258\n",
      "Train Epoch: 7 [12600/28547 (44%)]\tLoss: 1.976518\n",
      "Train Epoch: 7 [12900/28547 (45%)]\tLoss: 2.084410\n",
      "Train Epoch: 7 [13200/28547 (46%)]\tLoss: 2.230159\n",
      "Train Epoch: 7 [13500/28547 (47%)]\tLoss: 2.071826\n",
      "Train Epoch: 7 [13800/28547 (48%)]\tLoss: 2.130528\n",
      "Train Epoch: 7 [14100/28547 (49%)]\tLoss: 2.083202\n",
      "Train Epoch: 7 [14400/28547 (50%)]\tLoss: 2.153054\n",
      "Train Epoch: 7 [14700/28547 (51%)]\tLoss: 2.178835\n",
      "Train Epoch: 7 [15000/28547 (53%)]\tLoss: 2.112104\n",
      "Train Epoch: 7 [15300/28547 (54%)]\tLoss: 2.171442\n",
      "Train Epoch: 7 [15600/28547 (55%)]\tLoss: 2.080920\n",
      "Train Epoch: 7 [15900/28547 (56%)]\tLoss: 2.020438\n",
      "Train Epoch: 7 [16200/28547 (57%)]\tLoss: 2.113885\n",
      "Train Epoch: 7 [16500/28547 (58%)]\tLoss: 2.008738\n",
      "Train Epoch: 7 [16800/28547 (59%)]\tLoss: 2.073578\n",
      "Train Epoch: 7 [17100/28547 (60%)]\tLoss: 2.040591\n",
      "Train Epoch: 7 [17400/28547 (61%)]\tLoss: 2.126843\n",
      "Train Epoch: 7 [17700/28547 (62%)]\tLoss: 2.044893\n",
      "Train Epoch: 7 [18000/28547 (63%)]\tLoss: 2.099638\n",
      "Train Epoch: 7 [18300/28547 (64%)]\tLoss: 1.984966\n",
      "Train Epoch: 7 [18600/28547 (65%)]\tLoss: 2.194270\n",
      "Train Epoch: 7 [18900/28547 (66%)]\tLoss: 2.306948\n",
      "Train Epoch: 7 [19200/28547 (67%)]\tLoss: 2.058621\n",
      "Train Epoch: 7 [19500/28547 (68%)]\tLoss: 2.062948\n",
      "Train Epoch: 7 [19800/28547 (69%)]\tLoss: 2.136105\n",
      "Train Epoch: 7 [20100/28547 (70%)]\tLoss: 1.960131\n",
      "Train Epoch: 7 [20400/28547 (71%)]\tLoss: 2.172383\n",
      "Train Epoch: 7 [20700/28547 (73%)]\tLoss: 2.044351\n",
      "Train Epoch: 7 [21000/28547 (74%)]\tLoss: 2.139843\n",
      "Train Epoch: 7 [21300/28547 (75%)]\tLoss: 2.065522\n",
      "Train Epoch: 7 [21600/28547 (76%)]\tLoss: 2.174767\n",
      "Train Epoch: 7 [21900/28547 (77%)]\tLoss: 2.204880\n",
      "Train Epoch: 7 [22200/28547 (78%)]\tLoss: 2.276535\n",
      "Train Epoch: 7 [22500/28547 (79%)]\tLoss: 2.068523\n",
      "Train Epoch: 7 [22800/28547 (80%)]\tLoss: 2.173499\n",
      "Train Epoch: 7 [23100/28547 (81%)]\tLoss: 2.121743\n",
      "Train Epoch: 7 [23400/28547 (82%)]\tLoss: 2.136863\n",
      "Train Epoch: 7 [23700/28547 (83%)]\tLoss: 2.055729\n",
      "Train Epoch: 7 [24000/28547 (84%)]\tLoss: 2.050368\n",
      "Train Epoch: 7 [24300/28547 (85%)]\tLoss: 2.023350\n",
      "Train Epoch: 7 [24600/28547 (86%)]\tLoss: 2.033952\n",
      "Train Epoch: 7 [24900/28547 (87%)]\tLoss: 2.338515\n",
      "Train Epoch: 7 [25200/28547 (88%)]\tLoss: 2.141338\n",
      "Train Epoch: 7 [25500/28547 (89%)]\tLoss: 2.041109\n",
      "Train Epoch: 7 [25800/28547 (90%)]\tLoss: 1.993758\n",
      "Train Epoch: 7 [26100/28547 (91%)]\tLoss: 2.009032\n",
      "Train Epoch: 7 [26400/28547 (92%)]\tLoss: 2.052376\n",
      "Train Epoch: 7 [26700/28547 (94%)]\tLoss: 1.998424\n",
      "Train Epoch: 7 [27000/28547 (95%)]\tLoss: 2.084723\n",
      "Train Epoch: 7 [27300/28547 (96%)]\tLoss: 2.327806\n",
      "Train Epoch: 7 [27600/28547 (97%)]\tLoss: 2.082819\n",
      "Train Epoch: 7 [27900/28547 (98%)]\tLoss: 2.169513\n",
      "Train Epoch: 7 [28200/28547 (99%)]\tLoss: 2.200278\n",
      "Train Epoch: 7 [28500/28547 (100%)]\tLoss: 2.125110\n",
      "\n",
      "Evaluating…\n",
      "Test set: Average loss: 2.1534\n",
      "Average Greedy CER: 1.000000 Average Greedy WER: 1.0000\n",
      "\n",
      "Average Beam CER: 1.104568 Average Beam WER: 3.0295\n",
      "\n",
      "Train Epoch: 8 [0/28547 (0%)]\tLoss: 2.143107\n",
      "Train Epoch: 8 [300/28547 (1%)]\tLoss: 2.127146\n",
      "Train Epoch: 8 [600/28547 (2%)]\tLoss: 2.111860\n",
      "Train Epoch: 8 [900/28547 (3%)]\tLoss: 2.065032\n",
      "Train Epoch: 8 [1200/28547 (4%)]\tLoss: 2.159772\n",
      "Train Epoch: 8 [1500/28547 (5%)]\tLoss: 2.232451\n",
      "Train Epoch: 8 [1800/28547 (6%)]\tLoss: 2.094676\n",
      "Train Epoch: 8 [2100/28547 (7%)]\tLoss: 2.127198\n",
      "Train Epoch: 8 [2400/28547 (8%)]\tLoss: 2.074362\n",
      "Train Epoch: 8 [2700/28547 (9%)]\tLoss: 2.203152\n",
      "Train Epoch: 8 [3000/28547 (11%)]\tLoss: 2.081619\n",
      "Train Epoch: 8 [3300/28547 (12%)]\tLoss: 2.141565\n",
      "Train Epoch: 8 [3600/28547 (13%)]\tLoss: 2.096076\n",
      "Train Epoch: 8 [3900/28547 (14%)]\tLoss: 2.032256\n",
      "Train Epoch: 8 [4200/28547 (15%)]\tLoss: 2.179034\n",
      "Train Epoch: 8 [4500/28547 (16%)]\tLoss: 2.157813\n",
      "Train Epoch: 8 [4800/28547 (17%)]\tLoss: 2.064899\n",
      "Train Epoch: 8 [5100/28547 (18%)]\tLoss: 2.063079\n",
      "Train Epoch: 8 [5400/28547 (19%)]\tLoss: 2.147959\n",
      "Train Epoch: 8 [5700/28547 (20%)]\tLoss: 2.187971\n",
      "Train Epoch: 8 [6000/28547 (21%)]\tLoss: 2.061764\n",
      "Train Epoch: 8 [6300/28547 (22%)]\tLoss: 2.229440\n",
      "Train Epoch: 8 [6600/28547 (23%)]\tLoss: 2.036528\n",
      "Train Epoch: 8 [6900/28547 (24%)]\tLoss: 2.079948\n",
      "Train Epoch: 8 [7200/28547 (25%)]\tLoss: 2.057855\n",
      "Train Epoch: 8 [7500/28547 (26%)]\tLoss: 2.138552\n",
      "Train Epoch: 8 [7800/28547 (27%)]\tLoss: 2.076564\n",
      "Train Epoch: 8 [8100/28547 (28%)]\tLoss: 2.132181\n",
      "Train Epoch: 8 [8400/28547 (29%)]\tLoss: 2.083742\n",
      "Train Epoch: 8 [8700/28547 (30%)]\tLoss: 2.028111\n",
      "Train Epoch: 8 [9000/28547 (32%)]\tLoss: 2.155102\n",
      "Train Epoch: 8 [9300/28547 (33%)]\tLoss: 2.196316\n",
      "Train Epoch: 8 [9600/28547 (34%)]\tLoss: 2.081770\n",
      "Train Epoch: 8 [9900/28547 (35%)]\tLoss: 2.151488\n",
      "Train Epoch: 8 [10200/28547 (36%)]\tLoss: 2.126369\n",
      "Train Epoch: 8 [10500/28547 (37%)]\tLoss: 2.124287\n",
      "Train Epoch: 8 [10800/28547 (38%)]\tLoss: 2.088926\n",
      "Train Epoch: 8 [11100/28547 (39%)]\tLoss: 2.099881\n",
      "Train Epoch: 8 [11400/28547 (40%)]\tLoss: 2.048724\n",
      "Train Epoch: 8 [11700/28547 (41%)]\tLoss: 2.084709\n",
      "Train Epoch: 8 [12000/28547 (42%)]\tLoss: 2.131268\n",
      "Train Epoch: 8 [12300/28547 (43%)]\tLoss: 1.978281\n",
      "Train Epoch: 8 [12600/28547 (44%)]\tLoss: 2.215217\n",
      "Train Epoch: 8 [12900/28547 (45%)]\tLoss: 2.089150\n",
      "Train Epoch: 8 [13200/28547 (46%)]\tLoss: 2.115792\n",
      "Train Epoch: 8 [13500/28547 (47%)]\tLoss: 2.264362\n",
      "Train Epoch: 8 [13800/28547 (48%)]\tLoss: 2.155632\n",
      "Train Epoch: 8 [14100/28547 (49%)]\tLoss: 2.047356\n",
      "Train Epoch: 8 [14400/28547 (50%)]\tLoss: 2.068790\n",
      "Train Epoch: 8 [14700/28547 (51%)]\tLoss: 2.021159\n",
      "Train Epoch: 8 [15000/28547 (53%)]\tLoss: 2.120131\n",
      "Train Epoch: 8 [15300/28547 (54%)]\tLoss: 2.086802\n",
      "Train Epoch: 8 [15600/28547 (55%)]\tLoss: 2.102276\n",
      "Train Epoch: 8 [15900/28547 (56%)]\tLoss: 2.047443\n",
      "Train Epoch: 8 [16200/28547 (57%)]\tLoss: 2.079529\n",
      "Train Epoch: 8 [16500/28547 (58%)]\tLoss: 2.102918\n",
      "Train Epoch: 8 [16800/28547 (59%)]\tLoss: 2.170181\n",
      "Train Epoch: 8 [17100/28547 (60%)]\tLoss: 2.147994\n",
      "Train Epoch: 8 [17400/28547 (61%)]\tLoss: 2.015569\n",
      "Train Epoch: 8 [17700/28547 (62%)]\tLoss: 2.067472\n",
      "Train Epoch: 8 [18000/28547 (63%)]\tLoss: 2.074565\n",
      "Train Epoch: 8 [18300/28547 (64%)]\tLoss: 2.174609\n",
      "Train Epoch: 8 [18600/28547 (65%)]\tLoss: 2.088822\n",
      "Train Epoch: 8 [18900/28547 (66%)]\tLoss: 2.071810\n",
      "Train Epoch: 8 [19200/28547 (67%)]\tLoss: 2.091635\n",
      "Train Epoch: 8 [19500/28547 (68%)]\tLoss: 2.181161\n",
      "Train Epoch: 8 [19800/28547 (69%)]\tLoss: 2.212081\n",
      "Train Epoch: 8 [20100/28547 (70%)]\tLoss: 2.204159\n",
      "Train Epoch: 8 [20400/28547 (71%)]\tLoss: 2.191531\n",
      "Train Epoch: 8 [20700/28547 (73%)]\tLoss: 2.104881\n",
      "Train Epoch: 8 [21000/28547 (74%)]\tLoss: 2.021300\n",
      "Train Epoch: 8 [21300/28547 (75%)]\tLoss: 2.187225\n",
      "Train Epoch: 8 [21600/28547 (76%)]\tLoss: 2.142418\n",
      "Train Epoch: 8 [21900/28547 (77%)]\tLoss: 2.154841\n",
      "Train Epoch: 8 [22200/28547 (78%)]\tLoss: 2.084092\n",
      "Train Epoch: 8 [22500/28547 (79%)]\tLoss: 2.089986\n",
      "Train Epoch: 8 [22800/28547 (80%)]\tLoss: 2.157664\n",
      "Train Epoch: 8 [23100/28547 (81%)]\tLoss: 2.120077\n",
      "Train Epoch: 8 [23400/28547 (82%)]\tLoss: 2.085826\n",
      "Train Epoch: 8 [23700/28547 (83%)]\tLoss: 2.097731\n",
      "Train Epoch: 8 [24000/28547 (84%)]\tLoss: 2.138234\n",
      "Train Epoch: 8 [24300/28547 (85%)]\tLoss: 2.070107\n",
      "Train Epoch: 8 [24600/28547 (86%)]\tLoss: 2.173594\n",
      "Train Epoch: 8 [24900/28547 (87%)]\tLoss: 2.073659\n",
      "Train Epoch: 8 [25200/28547 (88%)]\tLoss: 2.169676\n",
      "Train Epoch: 8 [25500/28547 (89%)]\tLoss: 2.093698\n",
      "Train Epoch: 8 [25800/28547 (90%)]\tLoss: 2.115815\n",
      "Train Epoch: 8 [26100/28547 (91%)]\tLoss: 2.220330\n",
      "Train Epoch: 8 [26400/28547 (92%)]\tLoss: 2.132498\n",
      "Train Epoch: 8 [26700/28547 (94%)]\tLoss: 2.157815\n",
      "Train Epoch: 8 [27000/28547 (95%)]\tLoss: 2.117072\n",
      "Train Epoch: 8 [27300/28547 (96%)]\tLoss: 2.121092\n",
      "Train Epoch: 8 [27600/28547 (97%)]\tLoss: 2.080234\n",
      "Train Epoch: 8 [27900/28547 (98%)]\tLoss: 2.141929\n",
      "Train Epoch: 8 [28200/28547 (99%)]\tLoss: 2.143033\n",
      "Train Epoch: 8 [28500/28547 (100%)]\tLoss: 2.117440\n",
      "\n",
      "Evaluating…\n",
      "Test set: Average loss: 2.1531\n",
      "Average Greedy CER: 1.000000 Average Greedy WER: 1.0000\n",
      "\n",
      "Average Beam CER: 1.093165 Average Beam WER: 2.9825\n",
      "\n",
      "Train Epoch: 9 [0/28547 (0%)]\tLoss: 2.139809\n",
      "Train Epoch: 9 [300/28547 (1%)]\tLoss: 2.107397\n",
      "Train Epoch: 9 [600/28547 (2%)]\tLoss: 2.155426\n",
      "Train Epoch: 9 [900/28547 (3%)]\tLoss: 2.082812\n",
      "Train Epoch: 9 [1200/28547 (4%)]\tLoss: 2.056630\n",
      "Train Epoch: 9 [1500/28547 (5%)]\tLoss: 2.073685\n",
      "Train Epoch: 9 [1800/28547 (6%)]\tLoss: 2.133846\n",
      "Train Epoch: 9 [2100/28547 (7%)]\tLoss: 2.089272\n",
      "Train Epoch: 9 [2400/28547 (8%)]\tLoss: 1.992965\n",
      "Train Epoch: 9 [2700/28547 (9%)]\tLoss: 2.202944\n",
      "Train Epoch: 9 [3000/28547 (11%)]\tLoss: 2.049101\n",
      "Train Epoch: 9 [3300/28547 (12%)]\tLoss: 2.045556\n",
      "Train Epoch: 9 [3600/28547 (13%)]\tLoss: 2.112200\n",
      "Train Epoch: 9 [3900/28547 (14%)]\tLoss: 2.094603\n",
      "Train Epoch: 9 [4200/28547 (15%)]\tLoss: 2.070613\n",
      "Train Epoch: 9 [4500/28547 (16%)]\tLoss: 2.050987\n",
      "Train Epoch: 9 [4800/28547 (17%)]\tLoss: 2.088357\n",
      "Train Epoch: 9 [5100/28547 (18%)]\tLoss: 2.178109\n",
      "Train Epoch: 9 [5400/28547 (19%)]\tLoss: 2.041966\n",
      "Train Epoch: 9 [5700/28547 (20%)]\tLoss: 2.170032\n",
      "Train Epoch: 9 [6000/28547 (21%)]\tLoss: 2.110593\n",
      "Train Epoch: 9 [6300/28547 (22%)]\tLoss: 2.114615\n",
      "Train Epoch: 9 [6600/28547 (23%)]\tLoss: 2.122058\n",
      "Train Epoch: 9 [6900/28547 (24%)]\tLoss: 2.109750\n",
      "Train Epoch: 9 [7200/28547 (25%)]\tLoss: 1.989533\n",
      "Train Epoch: 9 [7500/28547 (26%)]\tLoss: 2.018596\n",
      "Train Epoch: 9 [7800/28547 (27%)]\tLoss: 2.281850\n",
      "Train Epoch: 9 [8100/28547 (28%)]\tLoss: 2.091449\n",
      "Train Epoch: 9 [8400/28547 (29%)]\tLoss: 2.096877\n",
      "Train Epoch: 9 [8700/28547 (30%)]\tLoss: 2.212912\n",
      "Train Epoch: 9 [9000/28547 (32%)]\tLoss: 2.079477\n",
      "Train Epoch: 9 [9300/28547 (33%)]\tLoss: 2.138513\n",
      "Train Epoch: 9 [9600/28547 (34%)]\tLoss: 2.001493\n",
      "Train Epoch: 9 [9900/28547 (35%)]\tLoss: 2.017739\n",
      "Train Epoch: 9 [10200/28547 (36%)]\tLoss: 2.026159\n",
      "Train Epoch: 9 [10500/28547 (37%)]\tLoss: 2.139982\n",
      "Train Epoch: 9 [10800/28547 (38%)]\tLoss: 2.029205\n",
      "Train Epoch: 9 [11100/28547 (39%)]\tLoss: 2.090078\n",
      "Train Epoch: 9 [11400/28547 (40%)]\tLoss: 2.148699\n",
      "Train Epoch: 9 [11700/28547 (41%)]\tLoss: 2.227156\n",
      "Train Epoch: 9 [12000/28547 (42%)]\tLoss: 2.057371\n",
      "Train Epoch: 9 [12300/28547 (43%)]\tLoss: 2.524696\n",
      "Train Epoch: 9 [12600/28547 (44%)]\tLoss: 1.997524\n",
      "Train Epoch: 9 [12900/28547 (45%)]\tLoss: 2.101697\n",
      "Train Epoch: 9 [13200/28547 (46%)]\tLoss: 2.093453\n",
      "Train Epoch: 9 [13500/28547 (47%)]\tLoss: 2.089211\n",
      "Train Epoch: 9 [13800/28547 (48%)]\tLoss: 2.210435\n",
      "Train Epoch: 9 [14100/28547 (49%)]\tLoss: 2.077216\n",
      "Train Epoch: 9 [14400/28547 (50%)]\tLoss: 2.094170\n",
      "Train Epoch: 9 [14700/28547 (51%)]\tLoss: 2.048573\n",
      "Train Epoch: 9 [15000/28547 (53%)]\tLoss: 2.212778\n",
      "Train Epoch: 9 [15300/28547 (54%)]\tLoss: 1.934778\n",
      "Train Epoch: 9 [15600/28547 (55%)]\tLoss: 2.029344\n",
      "Train Epoch: 9 [15900/28547 (56%)]\tLoss: 2.097388\n",
      "Train Epoch: 9 [16200/28547 (57%)]\tLoss: 2.035422\n",
      "Train Epoch: 9 [16500/28547 (58%)]\tLoss: 1.940678\n",
      "Train Epoch: 9 [16800/28547 (59%)]\tLoss: 2.103728\n",
      "Train Epoch: 9 [17100/28547 (60%)]\tLoss: 2.103627\n",
      "Train Epoch: 9 [17400/28547 (61%)]\tLoss: 2.130689\n",
      "Train Epoch: 9 [17700/28547 (62%)]\tLoss: 2.118526\n",
      "Train Epoch: 9 [18000/28547 (63%)]\tLoss: 2.059345\n",
      "Train Epoch: 9 [18300/28547 (64%)]\tLoss: 2.025520\n",
      "Train Epoch: 9 [18600/28547 (65%)]\tLoss: 2.014568\n",
      "Train Epoch: 9 [18900/28547 (66%)]\tLoss: 2.309286\n",
      "Train Epoch: 9 [19200/28547 (67%)]\tLoss: 2.025451\n",
      "Train Epoch: 9 [19500/28547 (68%)]\tLoss: 2.061907\n",
      "Train Epoch: 9 [19800/28547 (69%)]\tLoss: 2.076827\n",
      "Train Epoch: 9 [20100/28547 (70%)]\tLoss: 2.083360\n",
      "Train Epoch: 9 [20400/28547 (71%)]\tLoss: 2.143432\n",
      "Train Epoch: 9 [20700/28547 (73%)]\tLoss: 2.086713\n",
      "Train Epoch: 9 [21000/28547 (74%)]\tLoss: 2.172590\n",
      "Train Epoch: 9 [21300/28547 (75%)]\tLoss: 1.985100\n",
      "Train Epoch: 9 [21600/28547 (76%)]\tLoss: 2.106258\n",
      "Train Epoch: 9 [21900/28547 (77%)]\tLoss: 2.195543\n",
      "Train Epoch: 9 [22200/28547 (78%)]\tLoss: 2.070121\n",
      "Train Epoch: 9 [22500/28547 (79%)]\tLoss: 2.098331\n",
      "Train Epoch: 9 [22800/28547 (80%)]\tLoss: 2.009987\n",
      "Train Epoch: 9 [23100/28547 (81%)]\tLoss: 2.017122\n",
      "Train Epoch: 9 [23400/28547 (82%)]\tLoss: 2.023303\n",
      "Train Epoch: 9 [23700/28547 (83%)]\tLoss: 2.035367\n",
      "Train Epoch: 9 [24000/28547 (84%)]\tLoss: 2.172427\n",
      "Train Epoch: 9 [24300/28547 (85%)]\tLoss: 2.092716\n",
      "Train Epoch: 9 [24600/28547 (86%)]\tLoss: 2.071869\n",
      "Train Epoch: 9 [24900/28547 (87%)]\tLoss: 2.189781\n",
      "Train Epoch: 9 [25200/28547 (88%)]\tLoss: 2.074861\n",
      "Train Epoch: 9 [25500/28547 (89%)]\tLoss: 2.092460\n",
      "Train Epoch: 9 [25800/28547 (90%)]\tLoss: 2.170369\n",
      "Train Epoch: 9 [26100/28547 (91%)]\tLoss: 2.155577\n",
      "Train Epoch: 9 [26400/28547 (92%)]\tLoss: 2.150852\n",
      "Train Epoch: 9 [26700/28547 (94%)]\tLoss: 2.129844\n",
      "Train Epoch: 9 [27000/28547 (95%)]\tLoss: 2.173701\n",
      "Train Epoch: 9 [27300/28547 (96%)]\tLoss: 2.076986\n",
      "Train Epoch: 9 [27600/28547 (97%)]\tLoss: 2.115147\n",
      "Train Epoch: 9 [27900/28547 (98%)]\tLoss: 2.152128\n",
      "Train Epoch: 9 [28200/28547 (99%)]\tLoss: 2.015848\n",
      "Train Epoch: 9 [28500/28547 (100%)]\tLoss: 2.038211\n",
      "\n",
      "Evaluating…\n",
      "Test set: Average loss: 2.1535\n",
      "Average Greedy CER: 1.000000 Average Greedy WER: 1.0000\n",
      "\n",
      "Average Beam CER: 1.114700 Average Beam WER: 3.0730\n",
      "\n",
      "Train Epoch: 10 [0/28547 (0%)]\tLoss: 2.114285\n",
      "Train Epoch: 10 [300/28547 (1%)]\tLoss: 2.148209\n",
      "Train Epoch: 10 [600/28547 (2%)]\tLoss: 2.037511\n",
      "Train Epoch: 10 [900/28547 (3%)]\tLoss: 2.021490\n",
      "Train Epoch: 10 [1200/28547 (4%)]\tLoss: 2.054150\n",
      "Train Epoch: 10 [1500/28547 (5%)]\tLoss: 2.150076\n",
      "Train Epoch: 10 [1800/28547 (6%)]\tLoss: 2.112388\n",
      "Train Epoch: 10 [2100/28547 (7%)]\tLoss: 2.100818\n",
      "Train Epoch: 10 [2400/28547 (8%)]\tLoss: 2.016856\n",
      "Train Epoch: 10 [2700/28547 (9%)]\tLoss: 2.150392\n",
      "Train Epoch: 10 [3000/28547 (11%)]\tLoss: 2.201477\n",
      "Train Epoch: 10 [3300/28547 (12%)]\tLoss: 1.985009\n",
      "Train Epoch: 10 [3600/28547 (13%)]\tLoss: 2.037057\n",
      "Train Epoch: 10 [3900/28547 (14%)]\tLoss: 2.199228\n",
      "Train Epoch: 10 [4200/28547 (15%)]\tLoss: 2.138687\n",
      "Train Epoch: 10 [4500/28547 (16%)]\tLoss: 2.115879\n",
      "Train Epoch: 10 [4800/28547 (17%)]\tLoss: 2.051136\n",
      "Train Epoch: 10 [5100/28547 (18%)]\tLoss: 2.034107\n",
      "Train Epoch: 10 [5400/28547 (19%)]\tLoss: 2.089452\n",
      "Train Epoch: 10 [5700/28547 (20%)]\tLoss: 2.121915\n",
      "Train Epoch: 10 [6000/28547 (21%)]\tLoss: 1.998244\n",
      "Train Epoch: 10 [6300/28547 (22%)]\tLoss: 2.059391\n",
      "Train Epoch: 10 [6600/28547 (23%)]\tLoss: 2.022377\n",
      "Train Epoch: 10 [6900/28547 (24%)]\tLoss: 2.097409\n",
      "Train Epoch: 10 [7200/28547 (25%)]\tLoss: 2.176394\n",
      "Train Epoch: 10 [7500/28547 (26%)]\tLoss: 2.076079\n",
      "Train Epoch: 10 [7800/28547 (27%)]\tLoss: 2.016832\n",
      "Train Epoch: 10 [8100/28547 (28%)]\tLoss: 2.078210\n",
      "Train Epoch: 10 [8400/28547 (29%)]\tLoss: 2.219976\n",
      "Train Epoch: 10 [8700/28547 (30%)]\tLoss: 2.169533\n",
      "Train Epoch: 10 [9000/28547 (32%)]\tLoss: 2.020785\n",
      "Train Epoch: 10 [9300/28547 (33%)]\tLoss: 2.010083\n",
      "Train Epoch: 10 [9600/28547 (34%)]\tLoss: 2.116971\n",
      "Train Epoch: 10 [9900/28547 (35%)]\tLoss: 2.133423\n",
      "Train Epoch: 10 [10200/28547 (36%)]\tLoss: 2.173350\n",
      "Train Epoch: 10 [10500/28547 (37%)]\tLoss: 2.098469\n",
      "Train Epoch: 10 [10800/28547 (38%)]\tLoss: 2.108450\n",
      "Train Epoch: 10 [11100/28547 (39%)]\tLoss: 2.191582\n",
      "Train Epoch: 10 [11400/28547 (40%)]\tLoss: 2.016743\n",
      "Train Epoch: 10 [11700/28547 (41%)]\tLoss: 2.045585\n",
      "Train Epoch: 10 [12000/28547 (42%)]\tLoss: 2.078422\n",
      "Train Epoch: 10 [12300/28547 (43%)]\tLoss: 2.069175\n",
      "Train Epoch: 10 [12600/28547 (44%)]\tLoss: 2.145633\n",
      "Train Epoch: 10 [12900/28547 (45%)]\tLoss: 2.212707\n",
      "Train Epoch: 10 [13200/28547 (46%)]\tLoss: 2.063044\n",
      "Train Epoch: 10 [13500/28547 (47%)]\tLoss: 2.137379\n",
      "Train Epoch: 10 [13800/28547 (48%)]\tLoss: 2.024831\n",
      "Train Epoch: 10 [14100/28547 (49%)]\tLoss: 2.123542\n",
      "Train Epoch: 10 [14400/28547 (50%)]\tLoss: 2.142224\n",
      "Train Epoch: 10 [14700/28547 (51%)]\tLoss: 2.152109\n",
      "Train Epoch: 10 [15000/28547 (53%)]\tLoss: 2.180743\n",
      "Train Epoch: 10 [15300/28547 (54%)]\tLoss: 2.036816\n",
      "Train Epoch: 10 [15600/28547 (55%)]\tLoss: 2.059105\n",
      "Train Epoch: 10 [15900/28547 (56%)]\tLoss: 2.122876\n",
      "Train Epoch: 10 [16200/28547 (57%)]\tLoss: 2.026368\n",
      "Train Epoch: 10 [16500/28547 (58%)]\tLoss: 2.154248\n",
      "Train Epoch: 10 [16800/28547 (59%)]\tLoss: 2.195779\n",
      "Train Epoch: 10 [17100/28547 (60%)]\tLoss: 2.148511\n",
      "Train Epoch: 10 [17400/28547 (61%)]\tLoss: 2.076377\n",
      "Train Epoch: 10 [17700/28547 (62%)]\tLoss: 2.067408\n",
      "Train Epoch: 10 [18000/28547 (63%)]\tLoss: 2.094106\n",
      "Train Epoch: 10 [18300/28547 (64%)]\tLoss: 2.105894\n",
      "Train Epoch: 10 [18600/28547 (65%)]\tLoss: 2.127395\n",
      "Train Epoch: 10 [18900/28547 (66%)]\tLoss: 2.095202\n",
      "Train Epoch: 10 [19200/28547 (67%)]\tLoss: 2.143398\n",
      "Train Epoch: 10 [19500/28547 (68%)]\tLoss: 2.054720\n",
      "Train Epoch: 10 [19800/28547 (69%)]\tLoss: 2.226174\n",
      "Train Epoch: 10 [20100/28547 (70%)]\tLoss: 2.113639\n",
      "Train Epoch: 10 [20400/28547 (71%)]\tLoss: 2.120196\n",
      "Train Epoch: 10 [20700/28547 (73%)]\tLoss: 2.065069\n",
      "Train Epoch: 10 [21000/28547 (74%)]\tLoss: 2.111680\n",
      "Train Epoch: 10 [21300/28547 (75%)]\tLoss: 2.034820\n",
      "Train Epoch: 10 [21600/28547 (76%)]\tLoss: 2.014910\n",
      "Train Epoch: 10 [21900/28547 (77%)]\tLoss: 2.104852\n",
      "Train Epoch: 10 [22200/28547 (78%)]\tLoss: 2.107845\n",
      "Train Epoch: 10 [22500/28547 (79%)]\tLoss: 1.963999\n",
      "Train Epoch: 10 [22800/28547 (80%)]\tLoss: 2.081465\n",
      "Train Epoch: 10 [23100/28547 (81%)]\tLoss: 2.134116\n",
      "Train Epoch: 10 [23400/28547 (82%)]\tLoss: 2.092976\n",
      "Train Epoch: 10 [23700/28547 (83%)]\tLoss: 2.360254\n",
      "Train Epoch: 10 [24000/28547 (84%)]\tLoss: 1.966111\n",
      "Train Epoch: 10 [24300/28547 (85%)]\tLoss: 2.068976\n",
      "Train Epoch: 10 [24600/28547 (86%)]\tLoss: 2.144771\n",
      "Train Epoch: 10 [24900/28547 (87%)]\tLoss: 2.186303\n",
      "Train Epoch: 10 [25200/28547 (88%)]\tLoss: 2.150540\n",
      "Train Epoch: 10 [25500/28547 (89%)]\tLoss: 2.101662\n",
      "Train Epoch: 10 [25800/28547 (90%)]\tLoss: 2.100034\n",
      "Train Epoch: 10 [26100/28547 (91%)]\tLoss: 2.068387\n",
      "Train Epoch: 10 [26400/28547 (92%)]\tLoss: 2.115179\n",
      "Train Epoch: 10 [26700/28547 (94%)]\tLoss: 1.992931\n",
      "Train Epoch: 10 [27000/28547 (95%)]\tLoss: 2.194045\n",
      "Train Epoch: 10 [27300/28547 (96%)]\tLoss: 2.057143\n",
      "Train Epoch: 10 [27600/28547 (97%)]\tLoss: 2.064155\n",
      "Train Epoch: 10 [27900/28547 (98%)]\tLoss: 2.011746\n",
      "Train Epoch: 10 [28200/28547 (99%)]\tLoss: 2.133849\n",
      "Train Epoch: 10 [28500/28547 (100%)]\tLoss: 2.167844\n",
      "\n",
      "Evaluating…\n",
      "Test set: Average loss: 2.1537\n",
      "Average Greedy CER: 1.000000 Average Greedy WER: 1.0000\n",
      "\n",
      "Average Beam CER: 1.122753 Average Beam WER: 3.1043\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = train_test(hparams, experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ---------------------------------------------------------------------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------------------------------------------------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.com/pigeongcc/speech-recognition/42f087c969e5452f9adada1a8f830f74\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     cer_beam [10]               : (0.9590429726890468, 1.186884465348938)\n",
      "COMET INFO:     cer_greedy [10]             : (0.9689930118208704, 1.0)\n",
      "COMET INFO:     test_loss [10]              : (2.1376038110501696, 2.159299903794341)\n",
      "COMET INFO:     train_learning_rate [95160] : (2.0000000000314905e-09, 0.0005)\n",
      "COMET INFO:     train_loss [95160]          : (1.7775334119796753, 7.485280513763428)\n",
      "COMET INFO:     wer_beam [10]               : (1.0003439704725359, 3.359682443682843)\n",
      "COMET INFO:     wer_greedy [10]             : (0.9995506937300831, 1.0)\n",
      "COMET INFO:   Others:\n",
      "COMET INFO:     Name : epochs:10-rnn_type:LSTM-rnn_dim:256-n_rnn_layers:7-n_cnn_layers:4\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     batch_size    : 3\n",
      "COMET INFO:     dropout       : 0.05\n",
      "COMET INFO:     epochs        : 10\n",
      "COMET INFO:     learning_rate : 0.0005\n",
      "COMET INFO:     n_class       : 28\n",
      "COMET INFO:     n_cnn_layers  : 4\n",
      "COMET INFO:     n_feats       : 256\n",
      "COMET INFO:     n_rnn_layers  : 7\n",
      "COMET INFO:     rnn_dim       : 256\n",
      "COMET INFO:     rnn_type      : LSTM\n",
      "COMET INFO:     stride        : 1\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     conda-environment-definition : 1\n",
      "COMET INFO:     conda-info                   : 1\n",
      "COMET INFO:     conda-specification          : 1\n",
      "COMET INFO:     environment details          : 1\n",
      "COMET INFO:     filename                     : 1\n",
      "COMET INFO:     git metadata                 : 1\n",
      "COMET INFO:     git-patch (uncompressed)     : 1 (17.53 KB)\n",
      "COMET INFO:     installed packages           : 1\n",
      "COMET INFO:     notebook                     : 1\n",
      "COMET INFO:     os packages                  : 1\n",
      "COMET INFO:     source_code                  : 1\n",
      "COMET INFO: \n",
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: torch. Metrics and hyperparameters can still be logged using Experiment.log_metrics() and Experiment.log_parameters()\n",
      "COMET INFO: Uploading metrics, params, and assets to Comet before program termination (may take several seconds)\n",
      "COMET INFO: The Python SDK has 3600 seconds to finish before aborting...\n"
     ]
    }
   ],
   "source": [
    "experiment.end()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"./models\"):\n",
    "    os.makedirs(\"./models\")\n",
    "\n",
    "def get_model_paths(experiment_name):\n",
    "    model_folder_path = f'./models/{experiment_name}'\n",
    "    model_path = f'{model_folder_path}/{experiment_name}.pth'\n",
    "    hparams_path = f'{model_folder_path}/{experiment_name}.json'\n",
    "    return model_folder_path, model_path, hparams_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(experiment_name):\n",
    "\tmodel_folder_path, model_path, hparams_path = get_model_paths(experiment_name)\n",
    "\n",
    "\t# create folders if not exist\n",
    "\tif not os.path.isdir(model_folder_path):\n",
    "\t\tif not os.path.isdir(\"./models\"):\n",
    "\t\t\tos.makedirs(\"./models\")\n",
    "\t\tos.makedirs(model_folder_path)\n",
    "\t\t\n",
    "\ttorch.save(model.state_dict(), model_path)\n",
    "\n",
    "\twith open(hparams_path, 'w') as f:\n",
    "\t\tjson.dump(hparams, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(experiment_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(experiment_name):\n",
    "    _, model_path, hparams_path = get_model_paths(experiment_name)\n",
    "    \n",
    "    device, _ = get_device()\n",
    "\n",
    "    with open(hparams_path, 'r') as f:\n",
    "        hparams = json.load(f)\n",
    "\n",
    "    if 'rnn_type' not in hparams:\n",
    "        hparams['rnn_type'] = \"GRU\"\n",
    "\n",
    "    model = SpeechRecognitionModel(\n",
    "        hparams['rnn_type'], hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
    "        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
    "        ).to(device)\n",
    "\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    return model, hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_name = f\"asr-lch-optim:adamw-scheduler:oncecycle-data:full-epochs:30\"\n",
    "model_loaded, hparams_loaded = load_model(experiment_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative log likelihood matrix shape: torch.Size([1, 709, 28])\n",
      "\n",
      "GREEDY DECODING\n",
      "Decoded indices:\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "\n",
      "Target (len 0): ['']\n",
      "Prediction (len 0): ['']\n",
      "\n",
      "BEAM SEARCH DECODING\n",
      "Top beam:\n",
      "[tensor([1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6,\n",
      "        1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6,\n",
      "        1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6,\n",
      "        1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1],\n",
      "       dtype=torch.int32)]\n",
      "\n",
      "Target (len 0): ['']\n",
      "Prediction (len 89): ['e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e']\n"
     ]
    }
   ],
   "source": [
    "infer(model_loaded, sample_path='data/Iskander/flac/10000-9999999-0000.flac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative log likelihood matrix shape: torch.Size([1, 791, 28])\n",
      "\n",
      "GREEDY DECODING\n",
      "Decoded indices:\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')\n",
      "\n",
      "Target (len 141): [\"well i'm convinced that the boarded up house mystery happened not earlier than april sixteenth eighteen sixty one and probably not much later\"]\n",
      "Prediction (len 0): ['']\n",
      "\n",
      "BEAM SEARCH DECODING\n",
      "Top beam:\n",
      "[tensor([1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6,\n",
      "        1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6,\n",
      "        1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6,\n",
      "        1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6, 1, 6,\n",
      "        1, 6, 1, 6, 1], dtype=torch.int32)]\n",
      "\n",
      "Target (len 141): [\"well i'm convinced that the boarded up house mystery happened not earlier than april sixteenth eighteen sixty one and probably not much later\"]\n",
      "Prediction (len 99): ['e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e']\n"
     ]
    }
   ],
   "source": [
    "infer(model_loaded, sample_idx=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a4c2d921764f83a11dfe9b316c38093c37b9ffd72d2b52ea0257d21758f06cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
