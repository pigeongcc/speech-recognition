{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchaudio.datasets.SPEECHCOMMANDS(root='./', url='speech_commands_v0.01', download=True, subset='training')\n",
    "valid_dataset = torchaudio.datasets.SPEECHCOMMANDS(root='./', url='speech_commands_v0.01', download=True, subset='validation')\n",
    "test_dataset = torchaudio.datasets.SPEECHCOMMANDS(root='./', url='speech_commands_v0.01', download=True, subset='testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51088 6798 6835\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset), len(valid_dataset), len(test_dataset), )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_map_str = \"\"\"\n",
    " a 0\n",
    " b 1\n",
    " c 2\n",
    " d 3\n",
    " e 4\n",
    " f 5\n",
    " g 6\n",
    " h 7\n",
    " i 8\n",
    " j 9\n",
    " k 10\n",
    " l 11\n",
    " m 12\n",
    " n 13\n",
    " o 14\n",
    " p 15\n",
    " q 16\n",
    " r 17\n",
    " s 18\n",
    " t 19\n",
    " u 20\n",
    " v 21\n",
    " w 22\n",
    " x 23\n",
    " y 24\n",
    " z 25\n",
    " \"\"\"\n",
    " \n",
    "class TextTransform:\n",
    "    \"\"\" Maps characters to their indices, and vice versa \"\"\"\n",
    "    def __init__(self):\n",
    "        self.char_map = {}\n",
    "        self.index_map = {}\n",
    "        for line in char_map_str.strip().split('\\n'):\n",
    "            ch, index = line.split()\n",
    "            self.char_map[ch] = int(index)\n",
    "            self.index_map[int(index)] = ch\n",
    "\n",
    "    def text_to_int(self, text: list[str]):\n",
    "        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
    "        int_sequence = []\n",
    "        for c in text:\n",
    "            ind = self.char_map[c]\n",
    "            int_sequence.append(ind)\n",
    "        return int_sequence\n",
    "\n",
    "    def int_to_text(self, labels: list[int]):\n",
    "        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
    "        string = []\n",
    "        for i in labels:\n",
    "            string.append(self.index_map[i])\n",
    "        return ''.join(string)\n",
    "\n",
    "\n",
    "# TODO: SpecAugment (masking augmentations)\n",
    "train_audio_transforms = nn.Sequential(\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
    "    # torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n",
    "    # torchaudio.transforms.TimeMasking(time_mask_param=35)\n",
    ")\n",
    "\n",
    "valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
    "\n",
    "text_transform = TextTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes --> [24, 4, 18] --> yes\n"
     ]
    }
   ],
   "source": [
    "# testing the code above\n",
    "word_start = \"yes\"\n",
    "index = text_transform.text_to_int(word_start)\n",
    "word_recovered = text_transform.int_to_text(index)\n",
    "\n",
    "print(word_start, \"-->\", index, \"-->\", word_recovered)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция __data_processing__ будет позже вызвана в __collate_fn__ дата лоадеров.\n",
    "\n",
    "Формат данных в датасете: tuple (wave, sample_rate, utterance (label), speaker id, utterance number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0025, -0.0021, -0.0017,  ..., -0.0030, -0.0033, -0.0031]]),\n",
       " 16000,\n",
       " 'bed',\n",
       " '004ae714',\n",
       " 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = train_dataset.__getitem__(n=2)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(data, data_type=\"train\"):\n",
    "    spectrograms = []\n",
    "    labels = []\n",
    "    input_lengths = []\n",
    "    label_lengths = []\n",
    "    for (waveform, _, utterance, _, _) in data:\n",
    "        if data_type == 'train':\n",
    "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        else:\n",
    "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        spectrograms.append(spec)\n",
    "        # labels are lists of integer character ids\n",
    "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
    "        labels.append(label)\n",
    "        # input_lengths, label_lengths are used in loss function\n",
    "        input_lengths.append(spec.shape[0]//2)\n",
    "        label_lengths.append(len(label))\n",
    "\n",
    "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    return spectrograms, labels, input_lengths, label_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.3406e-02, 1.1594e-02, 1.6736e-02,  ..., 1.7414e-02,\n",
       "            1.6212e-02, 2.0798e-02],\n",
       "           [7.2180e-02, 6.2425e-02, 9.0109e-02,  ..., 9.3760e-02,\n",
       "            8.7287e-02, 1.1198e-01],\n",
       "           ...,\n",
       "           [5.7900e-06, 2.8763e-06, 3.3100e-06,  ..., 2.9408e-06,\n",
       "            2.1657e-06, 3.0014e-06],\n",
       "           [9.8653e-06, 2.7193e-06, 1.2482e-06,  ..., 6.7003e-07,\n",
       "            5.9030e-07, 1.4782e-06],\n",
       "           [9.3147e-06, 2.3657e-07, 1.7895e-07,  ..., 2.5175e-07,\n",
       "            8.9278e-08, 2.5031e-06]]]]),\n",
       " tensor([[1., 4., 3.]]),\n",
       " [37],\n",
       " [3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing\n",
    "data_processing((sample,))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a4c2d921764f83a11dfe9b316c38093c37b9ffd72d2b52ea0257d21758f06cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
